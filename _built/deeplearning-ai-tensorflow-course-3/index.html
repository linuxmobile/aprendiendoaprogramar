<!doctype html><html domain=.com lang=en><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="" http-equiv=Content-Security-Policy><link href=/favicon.svg rel=icon type=image/svg+xml><meta content=#f9c412 name=theme-color><meta content="max-snippet:-1, max-image-preview: large, max-video-preview: -1" name=robots><title>TF 3 - NLP in TensorFlow | Note of Thi</title><meta content="TF 3 - NLP in TensorFlow | Note of Thi" prefix=og:http://ogp.me/ns# property=og:title><meta content="This is my note for the 3rd course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..." name=description><meta content="This is my note for the 3rd course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..." prefix=og:http://ogp.me/ns# property=og:description><meta content=summary_large_image name=twitter:card><meta content=@dinhanhthi name=twitter:site><meta content=@dinhanhthi name=twitter:creator><meta content=https://dinhanhthi.com/img_src/cover.png prefix=og:http://ogp.me/ns# property=og:image><meta content=article prefix=og:http://ogp.me/ns# property=og:type><link href=https://dinhanhthi.com/deeplearning-ai-tensorflow-course-3/ rel=canonical><meta content=https://dinhanhthi.com/deeplearning-ai-tensorflow-course-3/ prefix=og:http://ogp.me/ns# property=og:url><meta content=no-referrer-when-downgrade name=referrer><link href=/feed/feed.xml rel=alternate type=application/atom+xml title="ðŸ”¥ Anh-Thi DINH"><link href=/ rel=preconnect crossorigin><link href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css rel=stylesheet crossorigin=anonymous integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET><script src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js defer crossorigin=anonymous integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb></script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js defer crossorigin=anonymous integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl onload=renderMathInElement(document.body);></script><style>body {
      visibility: hidden;
      opacity: 0;
    }</style><noscript><style>body {
        visibility: visible;
        opacity: 1;
      }</style></noscript><script csp-hash>if (/Mac OS X/.test(navigator.userAgent))
      document
        .documentElement
        .classList
        .add('apple')</script><link href=/src/css/main.css rel=stylesheet type=text/css><style>@font-face{font-display:swap;font-family:'fontello';src:url(/fontello/font/fontello.eot?46432155);src:url(/fontello/font/fontello.eot?46432155#iefix) format('embedded-opentype'),url(/fontello/font/fontello.woff2?46432155) format('woff2'),url(/fontello/font/fontello.woff?46432155) format('woff'),url(/fontello/font/fontello.ttf?46432155) format('truetype'),url(/fontello/font/fontello.svg?46432155#fontello) format('svg');font-weight:400;font-style:normal}[class*=" icon-"]:before,[class^=icon-]:before{font-family:"fontello";font-style:normal;font-weight:400;speak:never;display:inline-block;text-decoration:inherit;width:1em;margin-right:.2em;text-align:center;font-variant:normal;text-transform:none;line-height:1em;margin-left:.2em;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.icon-doc:before{content:'\e800'}.icon-doc-add:before{content:'\e801'}.icon-stats:before{content:'\e802'}.icon-tags:before{content:'\e803'}.icon-like:before{content:'\e804'}.icon-cog-alt:before{content:'\e805'}.icon-project:before{content:'\e806'}.icon-mooc:before{content:'\e807'}.icon-dl:before{content:'\e808'}.icon-ml:before{content:'\e809'}.icon-python:before{content:'\e80a'}.icon-nlp:before{content:'\e80b'}.icon-r-lang:before{content:'\e80c'}.icon-skill:before{content:'\e80d'}.icon-js-solid:before{content:'\e80e'}.icon-game:before{content:'\e80f'}.icon-web:before{content:'\e810'}.icon-algo:before{content:'\e811'}.icon-mooc-solid:before{content:'\e812'}.icon-chatbot:before{content:'\e813'}.icon-web-dev:before{content:'\e814'}.icon-data:before{content:'\e815'}.icon-skill-solid:before{content:'\e816'}.icon-python-solid:before{content:'\e817'}.icon-project-solid:before{content:'\e818'}.icon-js:before{content:'\e819'}.icon-rocket:before{content:'\e81a'}.icon-ml-solid:before{content:'\e81b'}.icon-game-solid:before{content:'\e81c'}.icon-dl-solid:before{content:'\e81d'}.icon-nlp-solid:before{content:'\e81e'}.icon-web-solid:before{content:'\e81f'}.icon-algo-solid:before{content:'\e820'}.icon-puzzle-outline:before{content:'\e821'}.icon-ng:before{content:'\e822'}.icon-ok:before{content:'\e823'}.icon-link:before{content:'\e824'}.icon-down-circle:before{content:'\e826'}.icon-api:before{content:'\e828'}.icon-right-circle:before{content:'\e829'}.icon-down-open:before{content:'\e82a'}.icon-right-open:before{content:'\e82b'}.icon-copy:before{content:'\f0c5'}.icon-gamepad:before{content:'\f11b'}.icon-fork:before{content:'\f126'}.icon-mlops-solid:before{content:'\f135'}.icon-edu-solid:before{content:'\f19d'}.icon-others-solid:before{content:'\f1b3'}.icon-ds-solid:before{content:'\f1c0'}.icon-chart-area:before{content:'\f1fe'}.icon-chart-pie:before{content:'\f200'}.icon-ts:before{content:'\f201'}.icon-clone:before{content:'\f24d'}</style><body><script>function showTheme() {
        const btn = document.getElementById("toggle-dark-light");
        let toggleIcon = btn.firstElementChild;
        const currentTheme = localStorage.getItem("theme");
        if (currentTheme === "dark") {
          document
            .body
            .classList
            .toggle("dark-theme");
          toggleIcon.src = "/img_src/nav/sun.svg";
        } else if (currentTheme === "light") {
          document
            .body
            .classList
            .toggle("light-theme");
          toggleIcon.src = "/img_src/nav/moon.svg";
        }
      }
      function showContent() {
        document.body.style.visibility = 'visible';
        document.body.style.opacity = 1;
      }
      window.addEventListener('DOMContentLoaded', (event) => {
        showTheme();
        showContent();
      });</script><header class="wave-border wave-border-post"><nav><div id=nav><a href=/ class="nav-item no-effect"><img alt=home class=keep-original src=/img_src/nav/home.svg height=18 width=18> <span>Thi</span> </a><a href=/about/ class="nav-item no-effect"><img alt=about class=keep-original src=/img_src/nav/about.svg height=15 width=15> <span>About</span></a><div class=nav-search id=nav-search><form><input aria-label='search notes (press "/" to focus & "ESC" to lose)' autocomplete=off class=nav-search__input id=nav-search__input onfocusin=inFocus(this) placeholder='search notes (press "/" to focus & "ESC" to lose)' type=search></form><div id=nav-search__result-container style="display: none;"><ul id=nav-search__ul></ul><div id=nav-search__no-result style="display: none;"><p>No results found.</div></div></div><span class="nav-item no-effect nav-dark-light" href="" id=toggle-dark-light><img alt=light-mode class=keep-original src=/img_src/nav/moon.svg height=20 width=20> </span><a href=https://github.com/dinhanhthi class="nav-item no-effect nav-github" target=_blank><img alt=github class=keep-original src=/img_src/nav/github.svg height=20 width=20></a></div><div class=reading-progress-container><div id=reading-progress aria-hidden=true></div></div></nav><script>var divNavSearch=document.getElementById("nav-search"),divRes=document.getElementById("nav-search__result-container"),ulRes=document.getElementById("nav-search__ul");function inFocus(e){""!=e.value&&(divRes.style.display="block")}var isOnDiv=!1;divRes.addEventListener("mouseover",(function(){isOnDiv=!0})),divRes.addEventListener("mouseout",(function(){isOnDiv=!1}));var inputSearch=document.getElementById("nav-search__input");window.addEventListener("click",(function(){ulRes.getElementsByTagName("li").length>=1&&isOnDiv||(divRes.style.display="none")})),inputSearch.addEventListener("click",(e=>{e.stopPropagation()}));const addSelected=e=>{ulRes.querySelectorAll("li").forEach((e=>{e.classList.remove("selected")})),e.classList.add("selected")};var isInView=(e,t)=>{var n=e.offsetTop+e.offsetHeight-t.scrollTop>t.offsetHeight;return e.offsetTop<t.scrollTop?"above":n?"below":"in"};const updateScroll=(e,t)=>{e.offsetTop+e.offsetHeight-t.scrollTop>t.offsetHeight&&(t.scrollTop=e.offsetTop+e.offsetHeight-t.offsetHeight),e.offsetTop<t.scrollTop&&(t.scrollTop=e.offsetTop)};document.onkeydown=e=>{checkInInput=document.activeElement==inputSearch,"/"!==e.key||checkInInput||(e.stopPropagation(),e.preventDefault(),inputSearch.focus())},document.addEventListener("focusin",(e=>{divNavSearch.contains(e.target)||(divRes.style.display="none")})),inputSearch.onkeydown=e=>{if("Enter"===e.key){e.stopPropagation(),e.preventDefault();var t=ulRes.querySelector('li[class*="selected"]');window.location.href=t.getElementsByClassName("item__content")[0].firstChild.firstChild.href}"Escape"===e.key&&(divRes.style.display="none",inputSearch.blur())},divNavSearch.onkeydown=e=>{if(hasResult=ulRes.getElementsByTagName("li").length>=1,hasResult){["ArrowUp","ArrowDown"].indexOf(e.key)>-1&&e.preventDefault();var t=ulRes.firstChild,n=ulRes.lastChild,s=ulRes.querySelector('li[class*="selected"]');switch(e.key){case"ArrowUp":nextLi=s&&s!=t?s.previousSibling:n,addSelected(nextLi),s=nextLi,updateScroll(s,divRes);break;case"ArrowDown":nextLi=s&&s!=n?s.nextSibling:t,addSelected(nextLi),s=nextLi,updateScroll(s,divRes)}}};</script><div class=header-container><div class="header-logo post-layout"><img alt="TF 3 - NLP in TensorFlow" class=keep-original src=/img/header/tensorflow.svg height=55 width=55></div><h1>TF 3 - NLP in TensorFlow</h1><div id=more-info><div id=note-tag><a href=/tags/mooc/ id=category }>MOOC</a> <a href=/tags/nlp/ id=category }>NLP</a> <a href=/tags/deeplearning.ai/ id=category }>deeplearning.ai</a> <a href=/tags/deep-learning/ id=category }>Deep Learning</a> <a href=/tags/tensorflow/ id=category }>TensorFlow</a></div><div id=last-modified>Last modified 2 years ago / <a href=https://github.com/dinhanhthi/notes/edit/master/./posts/mooc/2020-09-14-deeplearning-ai-tensorflow-course-3.md>Edit on Github</a></div></div></div></header><main class="" id=main-wrapper><article class=post-container><div class="container mt-2 normal page-note"><div class="danger not-full-warning"><div class=warning-icon><img alt=Warning class=keep-original src=/img_src/icons/time.svg></div><div>This post was <strong>updated more than 1 year ago</strong>, some information may be outdated!</div></div><div class="toc toc-common toc-js"><div class=ol-container><div class=toc-heading>In this note</div><ol><li><a href=#tokenizing-%2B-padding>Tokenizing + padding</a><li><a href=#word-embeddings>Word embeddings</a><ol><li><a href=#imdb-review-dataset>IMDB review dataset</a><li><a href=#sarcasm-dataset>Sarcasm dataset</a></ol><li><a href=#pre-tokenized-datasets>Pre-tokenized datasets</a><li><a href=#sequence-models>Sequence models</a><ol><li><a href=#rnn-idea>RNN idea</a><li><a href=#lstm-idea>LSTM idea</a><li><a href=#with-vs-without-lstm>With vs without LSTM</a><li><a href=#using-a-convnet>Using a ConvNet</a><li><a href=#imdb-dataset>IMDB dataset</a></ol><li><a href=#sequence-models-and-literature>Sequence models and literature</a></ol></div></div><p>This is my note for the <a href=https://www.coursera.org/learn/natural-language-processing-tensorflow>3rd course</a> of <a href=https://www.coursera.org/specializations/tensorflow-in-practice>TensorFlow in Practice Specialization</a> given by <a href=http://deeplearning.ai/ >deeplearning.ai</a> and taught by Laurence Moroney on Coursera.<p>ðŸ‘‰ Check the codes <a href=https://github.com/dinhanhthi/deeplearning.ai-courses/tree/master/TensorFlow%20in%20Practice>on my Github</a>.<br>ðŸ‘‰ Official <a href=https://github.com/lmoroney/dlaicourse>notebooks</a> on Github.<p>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-1>course 1 - Intro to TensorFlow for AI, ML, DL</a>.<br>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-2>course 2 - CNN in TensorFlow</a>.<br>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-4>course 4 - Sequences, Time Series and Prediction</a>.<h2 id=tokenizing-%2B-padding tabindex=-1>Tokenizing + padding <a href=#tokenizing-%2B-padding class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-1/notebook_1_tokenizer_basic_examples.html>Tokenizer basic examples.</a><br>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-1/notebook_2_sarcasm_detection.html>Sarcasm detection</a>.<p class=noindent><ul><li>A common simple character encoding is ASCII,<li>We can encode each word as a number (token) -- <a href=https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer><code>Tokenizer</code></a>.<li>Tokenize words > build all the words to make a corpus > turn your sentences into lists of values based on these tokens. > manipulate these lists (make the same length, for example)</ul><pre class=language-python><code class=language-python><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer<br><br>sentences <span class="token operator">=</span> <span class="token punctuation">[</span><br>    <span class="token string">'i love my dog'</span><span class="token punctuation">,</span><br>    <span class="token string">'I, love my cat'</span><span class="token punctuation">,</span><br>    <span class="token string">'You love my dog so much!'</span><br><span class="token punctuation">]</span><br><br>tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>num_words <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> oov_token<span class="token operator">=</span><span class="token string">"&lt;OOV>"</span><span class="token punctuation">)</span><br>            <span class="token comment"># num_words: max of words to be tokenized & pick</span><br>            <span class="token comment">#   the most common 100 words.</span><br>            <span class="token comment"># More words, more accuracy, more time to train</span><br>            <span class="token comment"># oov_token: replace unseen words by "&lt;OOV>"</span><br>tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span> <span class="token comment"># fix texts based on tokens</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># indexing words</span><br>word_index <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>word_index<br><span class="token keyword">print</span><span class="token punctuation">(</span>word_index<span class="token punctuation">)</span><br><span class="token comment"># {'&lt;OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7, 'so': 8, 'much': 9}</span><br><span class="token comment"># "!", ",", capital, ... are removed</span></code></pre><p>ðŸ‘‰ <a href=https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer>tf.keras.preprocessing.text.Tokenizer</a><pre class=language-python><code class=language-python><span class="token comment"># encode sentences</span><br>sequences <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><br><span class="token keyword">print</span><span class="token punctuation">(</span>sequences<span class="token punctuation">)</span><br><span class="token comment"># [[4, 2, 3, 5],</span><br><span class="token comment">#  [4, 2, 3, 6],</span><br><span class="token comment">#  [7, 2, 3, 5, 8, 9]]</span><br><span class="token comment"># if a word is not in the word index, it will be lost in the text_to_sequences()</span></code></pre><p>ðŸ‘‰ <a href=https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences>tf.keras.preprocessing.sequence.pad_sequences</a><pre class=language-python><code class=language-python><span class="token comment"># make encoded sentences equal</span><br><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>sequence <span class="token keyword">import</span> pad_sequences<br><br>padded <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><br>                       maxlen<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"post"</span><span class="token punctuation">,</span> truncating<span class="token operator">=</span><span class="token string">"post"</span><span class="token punctuation">)</span><br>         <span class="token comment"># maxlen: max len of encoded sentence</span><br>         <span class="token comment"># value: value to be filld (default 0)</span><br>         <span class="token comment"># padding: add missing values at beginning or ending of sentence?</span><br>         <span class="token comment"># truncating: longer than maxlen? cut at beginning or ending?</span><br><span class="token keyword">print</span><span class="token punctuation">(</span>padded<span class="token punctuation">)</span><br><span class="token comment"># [[ 4  2  3  5 -1]</span><br><span class="token comment">#  [ 4  2  3  6 -1]</span><br><span class="token comment">#  [ 7  2  3  5  8]]</span></code></pre><p>ðŸ‘‰ <a href=https://rishabhmisra.github.io/publications/ >Sarcasm detection dataset.</a><pre class=language-python><code class=language-python><span class="token comment"># read json text</span><br><span class="token keyword">import</span> json<br><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"/tmp/sarcasm.json"</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span><br>    datastore <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span><br><br>sentences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br>labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br>urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br><span class="token keyword">for</span> item <span class="token keyword">in</span> datastore<span class="token punctuation">:</span><br>    sentences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">'headline'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>    labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">'is_sarcastic'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>    urls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">'article_link'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h2 id=word-embeddings tabindex=-1>Word embeddings <a href=#word-embeddings class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><p>ðŸ‘‰ <a href=https://projector.tensorflow.org/ >Embedding projector - visualization of high-dimensional data</a><br>ðŸ‘‰ <a href=http://ai.stanford.edu/~amaas/data/sentiment/ >Large Movie Review Dataset</a><h3 id=imdb-review-dataset tabindex=-1>IMDB review dataset <a href=#imdb-review-dataset class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_1_IMDB_reviews.html>Train IMDB review dataset</a>.<br>ðŸ‘‰ <a href=https://www.coursera.org/lecture/natural-language-processing-tensorflow/notebook-for-lesson-1-Q1Ln5>Video explain the code</a>.<p class=noindent><ul><li><strong>Word embeddings</strong> = the idea in which words and associated words are <em>clustered as vectors</em> in a multi-dimensional space. That allows words with similar meaning to have a similar representation.<li>The meaning of the words can come from labeling of the dataset.<ul><li><em>Example</em>: "dull" and "boring" show up a lot in negative reviews <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo>â‡’</mo></mrow><annotation encoding=application/x-tex>\Rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.3669em;></span><span class=mrel>â‡’</span></span></span></span></span> they have similar sentiments <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo>â‡’</mo></mrow><annotation encoding=application/x-tex>\Rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.3669em;></span><span class=mrel>â‡’</span></span></span></span></span> they are close to each other in the sentence <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo>â‡’</mo></mrow><annotation encoding=application/x-tex>\Rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.3669em;></span><span class=mrel>â‡’</span></span></span></span></span> thus their vectors will be similar <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo>â‡’</mo></mrow><annotation encoding=application/x-tex>\Rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.3669em;></span><span class=mrel>â‡’</span></span></span></span></span> NN train + learn these vectors + associating them with the labels to come up with what's called in embedding.</ul><li>The purpose of <em>embedding dimension</em> is the number of dimensions for the vector representing the word encoding.</ul><pre class=language-python><code class=language-python><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<br><span class="token keyword">print</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span> <span class="token comment"># check version of tensorflow</span><br><br><span class="token comment"># If you are using tf1, you need below code</span><br>tf<span class="token punctuation">.</span>enable_eager_execution<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># IMDB reviews dataset</span><br><span class="token keyword">import</span> tensorflow_datasets <span class="token keyword">as</span> tfds<br>imdb<span class="token punctuation">,</span> info <span class="token operator">=</span> tfds<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"imdb_reviews"</span><span class="token punctuation">,</span> with_info<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> as_supervised<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br><br>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> imdb<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> imdb<span class="token punctuation">[</span><span class="token string">'test'</span><span class="token punctuation">]</span><br><br><span class="token keyword">for</span> s<span class="token punctuation">,</span>l <span class="token keyword">in</span> train_data<span class="token punctuation">:</span> <span class="token comment"># "s" for sentences "l" for labels</span><br>    <span class="token comment"># The values for "s" and "l" are tensors</span><br>    <span class="token comment"># so we need to extracr their values</span><br>    training_sentences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf8'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    training_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>l<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># Prepare for the NN</span><br>vocab_size <span class="token operator">=</span> <span class="token number">10000</span><br>embedding_dim <span class="token operator">=</span> <span class="token number">16</span> <span class="token comment"># embedding to dim 16</span><br>max_length <span class="token operator">=</span> <span class="token number">120</span> <span class="token comment"># of each sentence</span><br>trunc_type<span class="token operator">=</span><span class="token string">'post'</span> <span class="token comment"># cut the last words</span><br>oov_tok <span class="token operator">=</span> <span class="token string">"&lt;OOV>"</span> <span class="token comment"># replace not-encoded words by this</span><br><br><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizer<br><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>sequence <span class="token keyword">import</span> pad_sequences<br><br>tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>num_words <span class="token operator">=</span> vocab_size<span class="token punctuation">,</span> oov_token<span class="token operator">=</span>oov_tok<span class="token punctuation">)</span><br>tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>training_sentences<span class="token punctuation">)</span><br>    <span class="token comment"># encoding the words</span><br>word_index <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>word_index<br>    <span class="token comment"># list of word index (built based on training set)</span><br>    <span class="token comment"># there may be many oov_tok in test set</span><br>sequences <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>training_sentences<span class="token punctuation">)</span><br>    <span class="token comment"># apply on sentences</span><br>padded <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span>maxlen<span class="token operator">=</span>max_length<span class="token punctuation">,</span> truncating<span class="token operator">=</span>trunc_type<span class="token punctuation">)</span><br>    <span class="token comment"># padding the sentences</span><br><br><span class="token comment"># apply to the test set</span><br>testing_sequences <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>testing_sentences<span class="token punctuation">)</span><br>testing_padded <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span>testing_sequences<span class="token punctuation">,</span>maxlen<span class="token operator">=</span>max_length<span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># Simple NN</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> input_length<span class="token operator">=</span>max_length<span class="token punctuation">)</span><span class="token punctuation">,</span><br>                              <span class="token comment"># The result of embedding will be a 2D array:</span><br>                              <span class="token comment"># length of sentence x embedding_dim</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment"># Alternatively (a little diff on speed and accuracy):</span><br>    <span class="token comment"># tf.keras.layers.GlobalAveragePooling1D()</span><br>    <span class="token comment">#   average across the vectors to flatten it out</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'binary_crossentropy'</span><span class="token punctuation">,</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># Training</span><br>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>padded<span class="token punctuation">,</span> training_labels_final<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>testing_padded<span class="token punctuation">,</span> testing_labels_final<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># the result</span><br>e <span class="token operator">=</span> model<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token comment"># get the result of the embedding layers</span><br>weights <span class="token operator">=</span> e<span class="token punctuation">.</span>get_weights<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><br><span class="token keyword">print</span><span class="token punctuation">(</span>weights<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># shape: (vocab_size, embedding_dim)</span></code></pre><p>If you wanna visualize the result (in 3D) with <a href=https://projector.tensorflow.org/ >Embedding projector</a>,<pre class=language-python><code class=language-python><span class="token keyword">import</span> io<br><br>out_v <span class="token operator">=</span> io<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'vecs.tsv'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><br>out_m <span class="token operator">=</span> io<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'meta.tsv'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><br><span class="token keyword">for</span> word_num <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">:</span><br>  word <span class="token operator">=</span> reverse_word_index<span class="token punctuation">[</span>word_num<span class="token punctuation">]</span><br>  embeddings <span class="token operator">=</span> weights<span class="token punctuation">[</span>word_num<span class="token punctuation">]</span><br>  out_m<span class="token punctuation">.</span>write<span class="token punctuation">(</span>word <span class="token operator">+</span> <span class="token string">"\n"</span><span class="token punctuation">)</span><br>  out_v<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> embeddings<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n"</span><span class="token punctuation">)</span><br>out_v<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><br>out_m<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br><span class="token keyword">try</span><span class="token punctuation">:</span><br>  <span class="token keyword">from</span> google<span class="token punctuation">.</span>colab <span class="token keyword">import</span> files<br><span class="token keyword">except</span> ImportError<span class="token punctuation">:</span><br>  <span class="token keyword">pass</span><br><span class="token keyword">else</span><span class="token punctuation">:</span><br>  files<span class="token punctuation">.</span>download<span class="token punctuation">(</span><span class="token string">'vecs.tsv'</span><span class="token punctuation">)</span><br>  files<span class="token punctuation">.</span>download<span class="token punctuation">(</span><span class="token string">'meta.tsv'</span><span class="token punctuation">)</span></code></pre><h3 id=sarcasm-dataset tabindex=-1>Sarcasm dataset <a href=#sarcasm-dataset class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_2_sacarsm.html>Train Sacarsm dataset</a>.<ul><li>In text data, it usually happens that the accuracy increase over the number of training but the loss increase sharply also. We can "play" with hyperparameter to see the effect.</ul><pre class=language-python><code class=language-python><span class="token comment"># Run this to ensure TensorFlow 2.x is used</span><br><span class="token keyword">try</span><span class="token punctuation">:</span><br>  <span class="token comment"># %tensorflow_version only exists in Colab.</span><br>  <span class="token operator">%</span>tensorflow_version <span class="token number">2</span><span class="token punctuation">.</span>x<br><span class="token keyword">except</span> Exception<span class="token punctuation">:</span><br>  <span class="token keyword">pass</span></code></pre><h2 id=pre-tokenized-datasets tabindex=-1>Pre-tokenized datasets <a href=#pre-tokenized-datasets class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><p>ðŸ‘‰ <a href=https://github.com/tensorflow/datasets/blob/master/docs/catalog/imdb_reviews.md>datasets/imdb_reviews.md at master Â· tensorflow/datasets</a><br>ðŸ‘‰ <a href=https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder>tfds.features.text.SubwordTextEncoder Â |Â  TensorFlow Datasets</a><br>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-2/notebook_3_pre-tokenizer.html>Pre-tokenizer example</a>.<br>ðŸ‘‰ <a href=https://www.coursera.org/lecture/natural-language-processing-tensorflow/notebook-for-lesson-3-piQXt>Video exaplain the codes</a>.<ul><li>There are someones who did the work (tokenization) for you.<li>Try on IMDB dataset that has been pre-tokenized.<li>The tokenization is done on <strong>subwords</strong>!<li>The sequence of words can be just important as their existence.</ul><pre class=language-python><code class=language-python><span class="token comment"># load imdb dataset from tensorflow</span><br><span class="token keyword">import</span> tensorflow_datasets <span class="token keyword">as</span> tfds<br>imdb<span class="token punctuation">,</span> info <span class="token operator">=</span> tfds<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"imdb_reviews/subwords8k"</span><span class="token punctuation">,</span> with_info<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> as_supervised<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br><br><span class="token comment"># extract train/test sets</span><br>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> imdb<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> imdb<span class="token punctuation">[</span><span class="token string">'test'</span><span class="token punctuation">]</span><br><br><span class="token comment"># take the tokernizer</span><br>tokenizer <span class="token operator">=</span> info<span class="token punctuation">.</span>features<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>encoder<br><br><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>subwords<span class="token punctuation">)</span><br><span class="token comment"># ['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_',...</span></code></pre><pre class=language-python><code class=language-python>sample_string <span class="token operator">=</span> <span class="token string">'TensorFlow, from basics to mastery'</span><br><br>tokenized_string <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>sample_string<span class="token punctuation">)</span><br><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'Tokenized string is {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>tokenized_string<span class="token punctuation">)</span><span class="token punctuation">)</span><br><span class="token comment"># Tokenized string is [6307, 2327, 4043, 2120, 2, 48, 4249, 4429, 7, 2652, 8050]</span><br><br>original_string <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>tokenized_string<span class="token punctuation">)</span><br><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'The original string: {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>original_string<span class="token punctuation">)</span><span class="token punctuation">)</span><br><span class="token comment"># The original string: TensorFlow, from basics to mastery</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># take a look on tokenized string</span><br><span class="token comment"># case sensitive + punctuation maintained</span><br><span class="token keyword">for</span> ts <span class="token keyword">in</span> tokenized_string<span class="token punctuation">:</span><br>  <span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'{} ----> {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>ts<span class="token punctuation">,</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">[</span>ts<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><br><span class="token comment"># 6307 ----> Ten</span><br><span class="token comment"># 2327 ----> sor</span><br><span class="token comment"># 4043 ----> Fl</span><br><span class="token comment"># ...</span></code></pre><ul><li>The code run quite long (4 minutes each epoch if using GPU on colab) because there are a lot of hyperparameters and sub-words.<li>Result: 50% acc & loss is decreasing but very small.<ul><li>Because we are using sub-words, not for-words -> they (sub-words) are nonsensical. -> they are only when we put them together in sequences -> <strong>learning from sequences would be a great way forward</strong> -> <strong>RNN</strong> (Recurrent Neural Networks)</ul></ul><h2 id=sequence-models tabindex=-1>Sequence models <a href=#sequence-models class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><ul><li>The relative ordering, the sequence of words, matters for the meaning of the sentence .<li>For NN to take into account for the <strong>ordering of the words</strong>: <strong>RNN</strong> (Recurrent Neural Networks), <strong>LSTM</strong> (Long short-term memory).<li><strong>Why not RNN but LSTM ?</strong> With RNN, the context is preserved from timstamp to timestamp BUT that may get lost in longer sentences <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo>â‡’</mo></mrow><annotation encoding=application/x-tex>\Rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.3669em;></span><span class=mrel>â‡’</span></span></span></span></span> LSTM gets better because it has cell state.<li><strong>Example of using LSTM</strong>: "<em>I grew up in Ireland, I went to school and at school, they made me learn how to speak...</em>" <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo>â‡’</mo></mrow><annotation encoding=application/x-tex>\Rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.3669em;></span><span class=mrel>â‡’</span></span></span></span></span> "speak" is the context and we go back to the beginning to catch "Ireland", then the next word could be "leanr how to speak <strong>Gaelic</strong>"!</ul><h3 id=rnn-idea tabindex=-1>RNN idea <a href=#rnn-idea class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>ðŸ‘‰ <a href=https://www.notion.so/dinhanhthi/Sequence-models-by-deeplearning-ai-427774f7b31846fdb17ac09cd2353fbe>Note of the course of sequence model</a>.<p class=noindent><ul><li>The usual NN, something like "f(data, labels)=rules" cannot take into account of sequences.<li><strong>An example of using sequences</strong>: Fibonacci sequence <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mo>â‡’</mo></mrow><annotation encoding=application/x-tex>\Rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.3669em;></span><span class=mrel>â‡’</span></span></span></span></span> the result of current function is the input of next function itself,...</ul><p><img alt="RNN basic idea" class="pop img-70" src=/img/post/mooc/tf/rnn-basic-idea.png><br><em>RNN basic idea (<a href=https://medium.com/@kangeugine/long-short-term-memory-lstm-concept-cb3283934359>source</a>).</em><h3 id=lstm-idea tabindex=-1>LSTM idea <a href=#lstm-idea class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>ðŸ‘‰ (Video) <a href="https://www.youtube.com/watch?v=8HyCNIVRbSU&feature=emb_title">Illustrated Guide to LSTM's and GRU's: A step by step explanation</a> & <a href=https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21>its article</a>.<p class=noindent><ul><li>Sometimes, the sequence context leads to lose information like the example of "Ireland" and "Gaelic" before.<li>LSTM has an additional pipeline called <strong>Cell State</strong>. It can pass through the network to impact it + help to keep context from earlier tokens relevance.</ul><p><img alt="LSTM basic idea" class="pop img-75" src=/img/post/mooc/tf/lstm-basic-idea.png><br><em>LSTM basic idea (image from the course).</em><pre class=language-python><code class=language-python><span class="token comment"># SINGLE LAYER LSTM</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>      <span class="token comment"># 64: #oututs desired (but the result may be different)</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_1_IMDB_subwords_8K_with_single_layer_LSTM.html>IMDB Subwords 8K with Single Layer LSTM</a><pre class=language-python><code class=language-python><span class="token comment"># MULTI PLAYER LSTM</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>      <span class="token comment"># return_sequences=True: required if we wanna feed LSTM into another one</span><br>      <span class="token comment"># It ensures that the output of LSTM match the desired inputs of the next one</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_2_IMDB_subwords_8K_with_multi_layer_LSTM.html>IMDB Subwords 8K with Multi Layer LSTM</a><p><img alt="1layer vs 2 later LSTM acc" class="pop img-90" src=/img/post/mooc/tf/1layer-vs-2layer-lstm.png><br><em>1 layer vs 2 layer LSTM accuracy after 50 epochs (image from the course). 2 layer is better (smoother) which makes us more confident about the model. The validation acc is sticked to 80% because we used 8000 sub-words taken from training set, so there may be many tokens from the test set that would be out of vocabulary.</em><h3 id=with-vs-without-lstm tabindex=-1>With vs without LSTM <a href=#with-vs-without-lstm class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><pre class=language-python><code class=language-python><span class="token comment"># WITHOUT LSTM (like previous section)</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span><br>                              input_length<span class="token operator">=</span>max_length<span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GlobalmaxPooling1D<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># WITH LSTM</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span><br>                              input_length<span class="token operator">=</span>max_length<span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><img alt="With vs without LSTM" class="pop img-90" src=/img/post/mooc/tf/with-vs-without-lstm.png><br><em>With vs without LSTM (image from the course). With LSTM is really better but there is still overfitting here.</em><h3 id=using-a-convnet tabindex=-1>Using a ConvNet <a href=#using-a-convnet class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>ðŸ‘‰ <a href=https://www.coursera.org/lecture/natural-language-processing-tensorflow/using-a-convolutional-network-fSE8o>Video explains the dimension</a>.<br>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_3_IMDB_subwords_8K_with_Conv.html>IMDB Subwords 8K with 1D Convolutional Layer</a>.<pre class=language-python><code class=language-python>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv1D<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GlobalAveragePooling1D<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><img alt="Using Convolution network." class="pop img-90" src=/img/post/mooc/tf/using-conv-net.png><br><em>Using Convolution network. (image from the course). It's really better but there is overfitting there.</em><h3 id=imdb-dataset tabindex=-1>IMDB dataset <a href=#imdb-dataset class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-3/notebook_4_IMDB_review_with_GRU.html>IMDB Reviews with GRU (and optional LSTM and Conv1D)</a>.<br>ðŸ‘‰ <a href=https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/NFvFd/going-back-to-the-imdb-dataset>Video compares the results</a>.<p>Try with 3 different choices:<p class=indent><ul><li><strong>Simple NN</strong>: 5s/epoch, 170K params, nice acc, overfitting.<li><strong>LSTM</strong>: 43s/epoch, 30K params, acc better, overfitting.<li><strong>GRU</strong> (Gated Recurrent Unit layer, a different type of RNN): 20s/epoch, 169K params, very good acc, overfitting.<li><strong>Conv1D</strong>: 6s/epoch, 171K params, good acc, overfitting.</ul><p><strong>Remark</strong>: <mark>With the texts, you'll probably get a bit more overfitting than you would have done with images.</mark> Because we have out of voca words in validation data.<h2 id=sequence-models-and-literature tabindex=-1>Sequence models and literature <a href=#sequence-models-and-literature class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><p>One application of sequence models: read text then <strong>generate another look-alike text</strong>.<p>ðŸ“™ <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_1_find_the_next_word_trained_from_a_song.html>Notebook 1</a> & <a href=https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/B80b0/notebook-for-lesson-1>explaining video</a>.<ul><li>How they predict a new word in the notebook? -> Check <a href=https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/LGBS2/predicting-a-word>this video</a>.</ul><pre class=language-python><code class=language-python>input_sequences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br><span class="token keyword">for</span> line <span class="token keyword">in</span> corpus<span class="token punctuation">:</span><br>	<span class="token comment"># convert each sentence to list of numbers</span><br>	token_list <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span>line<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><br>	<span class="token comment"># convert each list to n-gram sequence</span><br>	<span class="token comment"># eg. from [1,2,3,4,5]</span><br>	<span class="token comment"># 		to [1,2], [1,2,3], [1,2,3,4], [1,2,3,4,5]</span><br>	<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>token_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span><br>		n_gram_sequence <span class="token operator">=</span> token_list<span class="token punctuation">[</span><span class="token punctuation">:</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><br>		input_sequences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>n_gram_sequence<span class="token punctuation">)</span><br><br><span class="token comment"># pad sequences to the maximum length of all sentences</span><br>max_sequence_len <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> input_sequences<span class="token punctuation">]</span><span class="token punctuation">)</span><br>input_sequences <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pad_sequences<span class="token punctuation">(</span>input_sequences<span class="token punctuation">,</span> maxlen<span class="token operator">=</span>max_sequence_len<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'pre'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><br><span class="token comment"># create predictors and label</span><br><span class="token comment"># [0,0,1,2] -> 2 is label</span><br><span class="token comment"># [0,1,2,3] -> 3 is label</span><br><span class="token comment"># [1,2,3,4] -> 4 is label</span><br>xs<span class="token punctuation">,</span> labels <span class="token operator">=</span> input_sequences<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>input_sequences<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><br><br><span class="token comment"># one-hot encoding the labels (classification problem)</span><br>ys <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>to_categorical<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> num_classes<span class="token operator">=</span>total_words<span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python>model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Embedding<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> input_length<span class="token operator">=</span>max_sequence_len<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Bidirectional<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># take only 20 units (bi-direction) to train</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>xs<span class="token punctuation">,</span> ys<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python>seed_text <span class="token operator">=</span> <span class="token string">"Laurence went to dublin"</span><br>next_words <span class="token operator">=</span> <span class="token number">100</span><br><br><span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>next_words<span class="token punctuation">)</span><span class="token punctuation">:</span><br>	token_list <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span>seed_text<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><br>	<span class="token comment"># "went to dublin" -> [134, 13, 59]</span><br>	token_list <span class="token operator">=</span> pad_sequences<span class="token punctuation">(</span><span class="token punctuation">[</span>token_list<span class="token punctuation">]</span><span class="token punctuation">,</span> maxlen<span class="token operator">=</span>max_sequence_len<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'pre'</span><span class="token punctuation">)</span><br>	<span class="token comment">#  [0, 0, 0, 0, 0, 0, 0, 134, 13, 59]</span><br>	predicted <span class="token operator">=</span> model<span class="token punctuation">.</span>predict_classes<span class="token punctuation">(</span>token_list<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><br>	output_word <span class="token operator">=</span> <span class="token string">""</span><br>	<span class="token comment"># revert an index back to the word</span><br>	<span class="token keyword">for</span> word<span class="token punctuation">,</span> index <span class="token keyword">in</span> tokenizer<span class="token punctuation">.</span>word_index<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><br>		<span class="token keyword">if</span> index <span class="token operator">==</span> predicted<span class="token punctuation">:</span><br>			output_word <span class="token operator">=</span> word<br>			<span class="token keyword">break</span><br>	<span class="token comment"># add predicted word to the seed text and make another prediction</span><br>	seed_text <span class="token operator">+=</span> <span class="token string">" "</span> <span class="token operator">+</span> output_word<br><span class="token keyword">print</span><span class="token punctuation">(</span>seed_text<span class="token punctuation">)</span><br><span class="token comment"># all the words are predicted based on the probability</span><br><span class="token comment"># next one will be less certain than the previous</span><br><span class="token comment"># -> less meaningful</span></code></pre><ul><li>Using more words will help.</ul><p>ðŸ“™ <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_3_more_data_on_train.html>Notebook 3 (more data)</a><pre class=language-python><code class=language-python><span class="token comment"># read from a file</span><br>tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span><span class="token punctuation">)</span><br>data <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'/tmp/irish-lyrics-eof.txt'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><br>corpus <span class="token operator">=</span> data<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span></code></pre><p>A little changes from the previous,<pre class=language-python><code class=language-python>model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Embedding<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> input_length<span class="token operator">=</span>max_sequence_len<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Bidirectional<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span><span class="token number">150</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>total_words<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>adam <span class="token operator">=</span> Adam<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span> <span class="token comment"># customized optimizer</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>adam<span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br><span class="token comment">#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')</span><br>history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>xs<span class="token punctuation">,</span> ys<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><ul><li>Different convernges can create different poetry.<li>If we use one-hot for a very big corpus -> take a lot of RAM -> use <strong>character-based prediction</strong> -> #unique characters is far less than #unique words. -> <a href=https://www.tensorflow.org/tutorials/text/text_generation>notebook "Text generation with RNN"</a></ul><p>ðŸ“™ Notebook <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-3/week-4/notebook_4_using_lstm_write_shakespeare.html>Using LSTMs, see if you can write Shakespeare!</a></div><script type=application/ld+json>{
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": "TF 3 - NLP in TensorFlow",
        "image": [],
        "author": "Anh-Thi DINH",
        "url": "https://dinhanhthi.com/deeplearning-ai-tensorflow-course-3/",
        "mainEntityOfPage": "https://dinhanhthi.com/deeplearning-ai-tensorflow-course-3/",
        "datePublished": "14-09-2020",
        "dateModified": "2020-09-14",
        "description": "This is my note for the 3rd course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..."
      }</script></article></main><footer><a href=/ target=_blank>Thi Â Â©Â  2022 </a>Â â€¢Â  <a href=/about-the-notes/ >About this site </a>Â â€¢Â  <a href=https://photos.app.goo.gl/9OVEkdTjmtRPg7vC3 target=_blank>My sketches </a>Â â€¢Â  <a href=https://goo.gl/photos/yQXdQws1LLS16x5v5 target=_blank>I cook </a>Â â€¢Â  <a href=/support-thi/ ><img alt="Support Thi" class=keep-original src=/img_src/icons/coffee.svg height=16 width=auto> Support Thi</a></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/elasticlunr/0.9.6/elasticlunr.min.js></script><script src=/src/js/components/clipboard.min.js></script><script src="/src/js/main.min.js?hash=d3b51a19b7" defer async></script><a href=/support-thi/ class="floating-button tooltip" id=buy-me-a-coffee><img alt="Support Thi" class=keep-original src=/img_src/icons/coffee.svg> <span class=tooltiptext>Support Thi</span> </a><button class="floating-button tooltip" id=scroll-top-btn><img alt="Scroll to top" class=keep-original src=/img_src/to-top.webp> <span class=tooltiptext>Top</span></button>