<!doctype html><html domain=.com lang=en><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="" http-equiv=Content-Security-Policy><link href=/favicon.svg rel=icon type=image/svg+xml><meta content=#f9c412 name=theme-color><meta content="max-snippet:-1, max-image-preview: large, max-video-preview: -1" name=robots><title>TF 4 - Sequences, TS and Prediction | Note of Thi</title><meta content="TF 4 - Sequences, TS and Prediction | Note of Thi" prefix=og:http://ogp.me/ns# property=og:title><meta content="This is my note for the 4th course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..." name=description><meta content="This is my note for the 4th course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..." prefix=og:http://ogp.me/ns# property=og:description><meta content=summary_large_image name=twitter:card><meta content=@dinhanhthi name=twitter:site><meta content=@dinhanhthi name=twitter:creator><meta content=https://dinhanhthi.com/img_src/cover.png prefix=og:http://ogp.me/ns# property=og:image><meta content=article prefix=og:http://ogp.me/ns# property=og:type><link href=https://dinhanhthi.com/deeplearning-ai-tensorflow-course-4/ rel=canonical><meta content=https://dinhanhthi.com/deeplearning-ai-tensorflow-course-4/ prefix=og:http://ogp.me/ns# property=og:url><meta content=no-referrer-when-downgrade name=referrer><link href=/feed/feed.xml rel=alternate type=application/atom+xml title="ðŸ”¥ Anh-Thi DINH"><link href=/ rel=preconnect crossorigin><link href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css rel=stylesheet crossorigin=anonymous integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET><script src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js defer crossorigin=anonymous integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb></script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js defer crossorigin=anonymous integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl onload=renderMathInElement(document.body);></script><style>body {
      visibility: hidden;
      opacity: 0;
    }</style><noscript><style>body {
        visibility: visible;
        opacity: 1;
      }</style></noscript><script csp-hash>if (/Mac OS X/.test(navigator.userAgent))
      document
        .documentElement
        .classList
        .add('apple')</script><link href=/src/css/main.css rel=stylesheet type=text/css><style>@font-face{font-display:swap;font-family:'fontello';src:url(/fontello/font/fontello.eot?46432155);src:url(/fontello/font/fontello.eot?46432155#iefix) format('embedded-opentype'),url(/fontello/font/fontello.woff2?46432155) format('woff2'),url(/fontello/font/fontello.woff?46432155) format('woff'),url(/fontello/font/fontello.ttf?46432155) format('truetype'),url(/fontello/font/fontello.svg?46432155#fontello) format('svg');font-weight:400;font-style:normal}[class*=" icon-"]:before,[class^=icon-]:before{font-family:"fontello";font-style:normal;font-weight:400;speak:never;display:inline-block;text-decoration:inherit;width:1em;margin-right:.2em;text-align:center;font-variant:normal;text-transform:none;line-height:1em;margin-left:.2em;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.icon-doc:before{content:'\e800'}.icon-doc-add:before{content:'\e801'}.icon-stats:before{content:'\e802'}.icon-tags:before{content:'\e803'}.icon-like:before{content:'\e804'}.icon-cog-alt:before{content:'\e805'}.icon-project:before{content:'\e806'}.icon-mooc:before{content:'\e807'}.icon-dl:before{content:'\e808'}.icon-ml:before{content:'\e809'}.icon-python:before{content:'\e80a'}.icon-nlp:before{content:'\e80b'}.icon-r-lang:before{content:'\e80c'}.icon-skill:before{content:'\e80d'}.icon-js-solid:before{content:'\e80e'}.icon-game:before{content:'\e80f'}.icon-web:before{content:'\e810'}.icon-algo:before{content:'\e811'}.icon-mooc-solid:before{content:'\e812'}.icon-chatbot:before{content:'\e813'}.icon-web-dev:before{content:'\e814'}.icon-data:before{content:'\e815'}.icon-skill-solid:before{content:'\e816'}.icon-python-solid:before{content:'\e817'}.icon-project-solid:before{content:'\e818'}.icon-js:before{content:'\e819'}.icon-rocket:before{content:'\e81a'}.icon-ml-solid:before{content:'\e81b'}.icon-game-solid:before{content:'\e81c'}.icon-dl-solid:before{content:'\e81d'}.icon-nlp-solid:before{content:'\e81e'}.icon-web-solid:before{content:'\e81f'}.icon-algo-solid:before{content:'\e820'}.icon-puzzle-outline:before{content:'\e821'}.icon-ng:before{content:'\e822'}.icon-ok:before{content:'\e823'}.icon-link:before{content:'\e824'}.icon-down-circle:before{content:'\e826'}.icon-api:before{content:'\e828'}.icon-right-circle:before{content:'\e829'}.icon-down-open:before{content:'\e82a'}.icon-right-open:before{content:'\e82b'}.icon-copy:before{content:'\f0c5'}.icon-gamepad:before{content:'\f11b'}.icon-fork:before{content:'\f126'}.icon-mlops-solid:before{content:'\f135'}.icon-edu-solid:before{content:'\f19d'}.icon-others-solid:before{content:'\f1b3'}.icon-ds-solid:before{content:'\f1c0'}.icon-chart-area:before{content:'\f1fe'}.icon-chart-pie:before{content:'\f200'}.icon-ts:before{content:'\f201'}.icon-clone:before{content:'\f24d'}</style><body><script>function showTheme() {
        const btn = document.getElementById("toggle-dark-light");
        let toggleIcon = btn.firstElementChild;
        const currentTheme = localStorage.getItem("theme");
        if (currentTheme === "dark") {
          document
            .body
            .classList
            .toggle("dark-theme");
          toggleIcon.src = "/img_src/nav/sun.svg";
        } else if (currentTheme === "light") {
          document
            .body
            .classList
            .toggle("light-theme");
          toggleIcon.src = "/img_src/nav/moon.svg";
        }
      }
      function showContent() {
        document.body.style.visibility = 'visible';
        document.body.style.opacity = 1;
      }
      window.addEventListener('DOMContentLoaded', (event) => {
        showTheme();
        showContent();
      });</script><header class="wave-border wave-border-post"><nav><div id=nav><a href=/ class="nav-item no-effect"><img alt=home class=keep-original src=/img_src/nav/home.svg height=18 width=18> <span>Thi</span> </a><a href=/about/ class="nav-item no-effect"><img alt=about class=keep-original src=/img_src/nav/about.svg height=15 width=15> <span>About</span></a><div class=nav-search id=nav-search><form><input aria-label='search notes (press "/" to focus & "ESC" to lose)' autocomplete=off class=nav-search__input id=nav-search__input onfocusin=inFocus(this) placeholder='search notes (press "/" to focus & "ESC" to lose)' type=search></form><div id=nav-search__result-container style="display: none;"><ul id=nav-search__ul></ul><div id=nav-search__no-result style="display: none;"><p>No results found.</div></div></div><span class="nav-item no-effect nav-dark-light" href="" id=toggle-dark-light><img alt=light-mode class=keep-original src=/img_src/nav/moon.svg height=20 width=20> </span><a href=https://github.com/dinhanhthi class="nav-item no-effect nav-github" target=_blank><img alt=github class=keep-original src=/img_src/nav/github.svg height=20 width=20></a></div><div class=reading-progress-container><div id=reading-progress aria-hidden=true></div></div></nav><script>var divNavSearch=document.getElementById("nav-search"),divRes=document.getElementById("nav-search__result-container"),ulRes=document.getElementById("nav-search__ul");function inFocus(e){""!=e.value&&(divRes.style.display="block")}var isOnDiv=!1;divRes.addEventListener("mouseover",(function(){isOnDiv=!0})),divRes.addEventListener("mouseout",(function(){isOnDiv=!1}));var inputSearch=document.getElementById("nav-search__input");window.addEventListener("click",(function(){ulRes.getElementsByTagName("li").length>=1&&isOnDiv||(divRes.style.display="none")})),inputSearch.addEventListener("click",(e=>{e.stopPropagation()}));const addSelected=e=>{ulRes.querySelectorAll("li").forEach((e=>{e.classList.remove("selected")})),e.classList.add("selected")};var isInView=(e,t)=>{var n=e.offsetTop+e.offsetHeight-t.scrollTop>t.offsetHeight;return e.offsetTop<t.scrollTop?"above":n?"below":"in"};const updateScroll=(e,t)=>{e.offsetTop+e.offsetHeight-t.scrollTop>t.offsetHeight&&(t.scrollTop=e.offsetTop+e.offsetHeight-t.offsetHeight),e.offsetTop<t.scrollTop&&(t.scrollTop=e.offsetTop)};document.onkeydown=e=>{checkInInput=document.activeElement==inputSearch,"/"!==e.key||checkInInput||(e.stopPropagation(),e.preventDefault(),inputSearch.focus())},document.addEventListener("focusin",(e=>{divNavSearch.contains(e.target)||(divRes.style.display="none")})),inputSearch.onkeydown=e=>{if("Enter"===e.key){e.stopPropagation(),e.preventDefault();var t=ulRes.querySelector('li[class*="selected"]');window.location.href=t.getElementsByClassName("item__content")[0].firstChild.firstChild.href}"Escape"===e.key&&(divRes.style.display="none",inputSearch.blur())},divNavSearch.onkeydown=e=>{if(hasResult=ulRes.getElementsByTagName("li").length>=1,hasResult){["ArrowUp","ArrowDown"].indexOf(e.key)>-1&&e.preventDefault();var t=ulRes.firstChild,n=ulRes.lastChild,s=ulRes.querySelector('li[class*="selected"]');switch(e.key){case"ArrowUp":nextLi=s&&s!=t?s.previousSibling:n,addSelected(nextLi),s=nextLi,updateScroll(s,divRes);break;case"ArrowDown":nextLi=s&&s!=n?s.nextSibling:t,addSelected(nextLi),s=nextLi,updateScroll(s,divRes)}}};</script><div class=header-container><div class="header-logo post-layout"><img alt="TF 4 - Sequences, TS and Prediction" class=keep-original src=/img/header/tensorflow.svg height=55 width=55></div><h1>TF 4 - Sequences, TS and Prediction</h1><div id=more-info><div id=note-tag><a href=/tags/mooc/ id=category }>MOOC</a> <a href=/tags/time-series/ id=category }>Time Series</a> <a href=/tags/deeplearning.ai/ id=category }>deeplearning.ai</a> <a href=/tags/deep-learning/ id=category }>Deep Learning</a> <a href=/tags/tensorflow/ id=category }>TensorFlow</a></div><div id=last-modified>Last modified 2 years ago / <a href=https://github.com/dinhanhthi/notes/edit/master/./posts/mooc/2020-10-14-deeplearning-ai-tensorflow-course-4.md>Edit on Github</a></div></div></div></header><main class="" id=main-wrapper><article class=post-container><div class="container mt-2 normal page-note"><div class="danger not-full-warning"><div class=warning-icon><img alt=Warning class=keep-original src=/img_src/icons/time.svg></div><div>This post was <strong>updated more than 1 year ago</strong>, some information may be outdated!</div></div><div class="toc toc-common toc-js"><div class=ol-container><div class=toc-heading>In this note</div><ol><li><a href=#sequences-and-prediction>Sequences and prediction</a><ol><li><a href=#time-series>Time Series</a><li><a href=#train-%2F-validation-%2F-test>Train / Validation / Test</a><li><a href=#metrics>Metrics</a><li><a href=#moving-average-and-differencing>Moving average and differencing</a></ol><li><a href=#deep-nn-for-time-series>Deep NN for Time Series</a><ol><li><a href=#preparing-features-and-labels>Preparing features and labels</a></ol><li><a href=#sequence-bias>Sequence bias</a><li><a href=#feeding-windowed-datasets-into-nn>Feeding windowed datasets into NN</a><li><a href=#rnn-for-ts>RNN for TS</a><ol><li><a href=#shape-of-input-to-rnn>Shape of input to RNN</a><li><a href=#sequence-to-vector-rnn>Sequence to vector RNN</a><li><a href=#lambda-layer>Lambda layer</a><li><a href=#simple-rnn>Simple RNN</a><li><a href=#lstm>LSTM</a></ol><li><a href=#real-world-time-series-data>Real-world time series data</a></ol></div></div><p>This is my note for the <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/ >4th course</a> of <a href=https://www.coursera.org/specializations/tensorflow-in-practice>TensorFlow in Practice Specialization</a> given by <a href=http://deeplearning.ai/ >deeplearning.ai</a> and taught by Laurence Moroney on Coursera.<p>ðŸ‘‰ Check the codes <a href=https://github.com/dinhanhthi/deeplearning.ai-courses/tree/master/TensorFlow%20in%20Practice>on my Github</a>.<br>ðŸ‘‰ Official <a href=https://github.com/lmoroney/dlaicourse>notebooks</a> on Github.<p>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-1>course 1 - Intro to TensorFlow for AI, ML, DL</a>.<br>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-2>course 2 - CNN in TensorFlow</a>.<br>ðŸ‘‰ Go to <a href=/deeplearning-ai-tensorflow-course-3>course 3 - NLP in Tensorflow</a>.<p class=noindent><ul><li><strong>Sequence models</strong>: focus on <em>time series</em> (there are others) -- stock, weather,...<li>At the end, we wanna model <strong>sunspot actitivity cycles</strong> which is important to NASA and other space agencies.<li>Using RNN on time series data.</ul><h2 id=sequences-and-prediction tabindex=-1>Sequences and prediction <a href=#sequences-and-prediction class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><h3 id=time-series tabindex=-1>Time Series <a href=#time-series class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-1/notebook_1_introduction_to_time_series.html>introduction to time series</a>. + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/Pzp5K/introduction-to-time-series>explaining video</a>. => How to create synthetic time series data + plot them.<p class=noindent><ul><li>Time series is everywhere: stock prices, weather focasts, historical trends (Moore's law),...<li><strong>Univariate</strong> TS and <strong>Miltivariate</strong> TS.<li>Type of things can we do with ML over TS:<ul><li>Any thing has a time factor can be analysed using TS.<li>Predicting a forecasting (eg. birth & death in Japan -> predict future for retirement, immigration, impacts...).<li><strong>Imputation</strong>: project back into the past.<li>Fill holes in the data.<li>Nomalies detecction (website attacks).<li>Spot patterns (eg. speed recognition).</ul><li>Common patterns in TS:<ul><li><p><strong>Trend</strong>: a specific direcion that they're moving in.<p><img alt=Trend class=img-50 src=/img/post/mooc/tf/trend.png><li><p><strong>Seasonality</strong>: patterns repeat at predictable intervals (eg. active users for a website).<p><img alt=seasonality class=img-100 src=/img/post/mooc/tf/seasonality.png><li><p>Combinition of both <strong>trend</strong> and <strong>seasonality</strong>.<p><img alt=trend+seasonality class=img-50 src=/img/post/mooc/tf/trend_seasonality.png><li><p><strong>Stationary</strong> TS.<p><img alt=stationality class=img-50 src=/img/post/mooc/tf/stationality.png><li><p><strong>Autocorrelated</strong> TS: a time series is linearly related to a <em>lagged</em> version of itself.. There is no trend, no seasonality.<p><img alt=autocorrelation class=img-60 src=/img/post/mooc/tf/autocorrelation.png><li><p><strong>Multiple auto correlation</strong>.<p><img alt=multiple_autocorrelation class=img-60 src=/img/post/mooc/tf/multiple_autocorrelation.png><li><p>May be <strong>trend</strong> + <strong>seasonality</strong> + <strong>autorrelation</strong> + <strong>noise</strong>.<p><img alt=trend_seasonality_autocorrelation_noise class=img-60 src=/img/post/mooc/tf/trend_seasonality_autocorrelation_noise.png><li><p><strong>Non-stationary</strong> TS:<p><img alt=non_stationary class=img-60 src=/img/post/mooc/tf/non_stationary.png><br><em>In this case, we base just on the later data to predict the future (not on the whole data).</em></ul></ul><h3 id=train-%2F-validation-%2F-test tabindex=-1>Train / Validation / Test <a href=#train-%2F-validation-%2F-test class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><ul><li><p><strong>Fixed partitioning</strong> (this course focuses on) = splitting TS data into <strong>training period</strong>, <strong>validation period</strong> and <strong>test period</strong>.<ul><li><p>If TS is seasonal, we want each period contains the whole number of seasons.<p><img alt="Fixed partitioning" class=img-70 src=/img/post/mooc/tf/fixed_partitioning.png></ul><li><p>We can split + train + test to get a model and then <strong>re-train</strong> with the data <strong>containing also the test period</strong> so that the model is optimized! In that case, the test set comes from the future.<p><img alt="Fixed partitioning with test period comes from the future" class=img-70 src=/img/post/mooc/tf/fixed_partitioning_future_test.png><li><p><strong>Roll-forward partitioning</strong>: we start with a short training period and we gradually increase it (1 day at a time or 1 week at a time). At each iteration, we train the model on training period, use it to focast the following day/week in the validation period. = Fixed partitioning in a number of times!<p><img alt="Roll-forward partitioning" class=img-70 src=/img/post/mooc/tf/roll_forward_partitioning.png></ul><h3 id=metrics tabindex=-1>Metrics <a href=#metrics class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>For evaluating models:<pre class=language-python><code class=language-python>errors <span class="token operator">=</span> forecasts <span class="token operator">-</span> actual<br><br><span class="token comment"># Mean squared error (square to get rid of negative values)</span><br><span class="token comment"># Eg. Used if large errors are potentially dangerous</span><br>mse <span class="token operator">=</span> np<span class="token punctuation">.</span>square<span class="token punctuation">(</span>errors<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><br><span class="token comment"># Get back to the same scale to error</span><br>rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mse<span class="token punctuation">)</span><br><br><span class="token comment"># Mean absolute error (his favorite)</span><br><span class="token comment"># this doesn't penalize large errs as much as mse does,</span><br><span class="token comment"># used if loss is proportional to the size of err</span><br>mae <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>errors<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><br><br><span class="token comment"># Mean abs percentage err</span><br><span class="token comment"># idea of the size of err compared to the values</span><br>mape <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>errors <span class="token operator">/</span> x_valid<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># MAE with TF</span><br>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>mean_absolute_error<span class="token punctuation">(</span>x_valid<span class="token punctuation">,</span> naive_forecast<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id=moving-average-and-differencing tabindex=-1>Moving average and differencing <a href=#moving-average-and-differencing class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-1/notebook_2_forecasting.html>Forecasting</a>. + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/KVWrR/forecasting>explaining video</a>.<p><strong>Moving average</strong>: a simple forecasting method. Calculate the average of blue lines within a fixed "averaging windows".<ul><li>This can eliminate noises and doesn't anticipate trend or seasonality.<li>Depend on the "averaging window", it can give worse result than naive forecast.</ul><p><img alt="Moving average" class=img-70 src=/img/post/mooc/tf/moving_average.png><br><em>Take the average on each yellow window. MAE=7.14 (optimal is 4).</em><pre class=language-python><code class=language-python><span class="token keyword">def</span> <span class="token function">moving_average_forecast</span><span class="token punctuation">(</span>series<span class="token punctuation">,</span> window_size<span class="token punctuation">)</span><span class="token punctuation">:</span><br>    <span class="token string triple-quoted-string">"""Forecasts the mean of the last few values.<br>        If window_size=1, then this is equivalent to naive forecast"""</span><br>    forecast <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br>    <span class="token keyword">for</span> time <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>series<span class="token punctuation">)</span> <span class="token operator">-</span> window_size<span class="token punctuation">)</span><span class="token punctuation">:</span><br>    forecast<span class="token punctuation">.</span>append<span class="token punctuation">(</span>series<span class="token punctuation">[</span>time<span class="token punctuation">:</span>time <span class="token operator">+</span> window_size<span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>forecast<span class="token punctuation">)</span></code></pre><p><strong>Differencing</strong>: remove the trend and seasonality from the TS. We study on the differences between points and their previous neighbor in period.<p><img alt="Moving average on differenced time series" class=img-100 src=/img/post/mooc/tf/moving_avg_on_differenced_ts.jpg><br><em>Left image: we find the differencing of original values, then we find the average (orange line). Right image: restore the trend and seasonality. MAE=5.8 (optimal is 4).</em><p>Above method still get the noises (because we add the differencing to the previous noise). If we remove past noise using moving average on that.<p><img alt="Smoothing both past and present values" class=img-70 src=/img/post/mooc/tf/smoothing_both_past_present_values.png><br><em>Smoothing both past and present values. MAE=4.5 (optimal is 4).</em><p>Keep in mind before using Deep Learning, <mark>sometimes simple approaches just work fine!</mark><h2 id=deep-nn-for-time-series tabindex=-1>Deep NN for Time Series <a href=#deep-nn-for-time-series class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><h3 id=preparing-features-and-labels tabindex=-1>Preparing features and labels <a href=#preparing-features-and-labels class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><ul><li>We need to split our TS data into features and labels so that we can use them in ML algos.<li>In this case: features=#values in TS, label=next_value.<ul><li>Feature: window size and train to predict next value.<li>Ex: 30 days of values as features and next value as label.<li>Overtime, train ML to match 30 features to match a single label.</ul></ul><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-2/notebook_1_preparing_features_and_labels.html>Preparing features and labels</a>.<br>ðŸ‘‰ <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/TYErD/preparing-features-and-labels>Video explains how to split to features and labels from dataset</a>.<pre class=language-python><code class=language-python><span class="token keyword">def</span> <span class="token function">windowed_dataset</span><span class="token punctuation">(</span>series<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle_buffer<span class="token punctuation">)</span><span class="token punctuation">:</span><br>    dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_tensor_slices<span class="token punctuation">(</span>series<span class="token punctuation">)</span><br>    dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>window<span class="token punctuation">(</span>window_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> shift<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> drop_remainder<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br>    dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>flat_map<span class="token punctuation">(</span><span class="token keyword">lambda</span> window<span class="token punctuation">:</span> window<span class="token punctuation">.</span>batch<span class="token punctuation">(</span>window_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>shuffle_buffer<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> window<span class="token punctuation">:</span> <span class="token punctuation">(</span>window<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> window<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">.</span>prefetch<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><br>    <span class="token keyword">return</span> dataset</code></pre><div class=hsbox><div class=hs__title>Explain the codes</div><div class=hs__content><pre class=language-python><code class=language-python><span class="token comment"># create a very simple dataset</span><br>dataset <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><br>arr <span class="token operator">=</span> <span class="token punctuation">[</span>val<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> val <span class="token keyword">in</span> dataset<span class="token punctuation">]</span><br><span class="token keyword">print</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><br><span class="token comment"># [0, 1, 2, 3, 4, 5]</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># make equal (drop_remaninder) windows</span><br>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>window<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> shift<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> drop_remainder<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><br>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>flat_map<span class="token punctuation">(</span><span class="token keyword">lambda</span> window<span class="token punctuation">:</span> window<span class="token punctuation">.</span>batch<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    <span class="token comment"># instead of val.numpy for each val in each window</span><br><span class="token keyword">for</span> window <span class="token keyword">in</span> dataset<span class="token punctuation">:</span><br>    <span class="token keyword">print</span><span class="token punctuation">(</span>window<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><span class="token comment"># [0 1 2 3 4]</span><br><span class="token comment"># [1 2 3 4 5]</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># split the last value to be label</span><br>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> window<span class="token punctuation">:</span> <span class="token punctuation">(</span>window<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> window<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><span class="token comment"># [0 1 2 3] [4]</span><br><span class="token comment"># [1 2 3 4] [5]</span></code></pre><pre class=language-python><code class=language-python><span class="token comment"># shuffle</span><br>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>buffer_size<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">)</span><br><span class="token comment"># construct batch of 2</span><br>dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>prefetch<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><br><span class="token comment"># x =  [[1 2 3 4], [0 1 2 3]]</span><br><span class="token comment"># y =  [[5], [4]]</span></code></pre></div></div><h2 id=sequence-bias tabindex=-1>Sequence bias <a href=#sequence-bias class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><p>Sequence bias is when the order of things can impact the selection of things. <mark>It's ok to shuffle!</mark><h2 id=feeding-windowed-datasets-into-nn tabindex=-1>Feeding windowed datasets into NN <a href=#feeding-windowed-datasets-into-nn class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-2/notebook_2_1layer_NN_linear_reg.html>Single layer NN</a> + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/YERBd/more-on-single-layer-neural-network>video explains it</a>.<pre class=language-python><code class=language-python><span class="token comment"># Simple linear regression (1 layer NN)</span><br>dataset <span class="token operator">=</span> windowed_dataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle_buffer_size<span class="token punctuation">)</span><br>l0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">[</span>window_size<span class="token punctuation">]</span><span class="token punctuation">)</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>l0<span class="token punctuation">]</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">"mse"</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span>epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><br><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Layer weights {}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>l0<span class="token punctuation">.</span>get_weights<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><br>forecast <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br><br><span class="token keyword">for</span> time <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>series<span class="token punctuation">)</span> <span class="token operator">-</span> window_size<span class="token punctuation">)</span><span class="token punctuation">:</span><br>    forecast<span class="token punctuation">.</span>append<span class="token punctuation">(</span>model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>series<span class="token punctuation">[</span>time<span class="token punctuation">:</span>time <span class="token operator">+</span> window_size<span class="token punctuation">]</span><span class="token punctuation">[</span>np<span class="token punctuation">.</span>newaxis<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    <span class="token comment"># np.newaxis: reshape X to input dimension that used by the model</span><br><br>forecast <span class="token operator">=</span> forecast<span class="token punctuation">[</span>split_time<span class="token operator">-</span>window_size<span class="token punctuation">:</span><span class="token punctuation">]</span><br>results <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>forecast<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span></code></pre><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-2/notebook_3_DNN_TS.html>DNN with TS</a> + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/HecKT/deep-neural-network-training-tuning-and-prediction>video explains it</a>.<pre class=language-python><code class=language-python><span class="token comment"># A way to choose an optimal learning rate</span><br>lr_schedule <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>LearningRateScheduler<span class="token punctuation">(</span><br>    <span class="token keyword">lambda</span> epoch<span class="token punctuation">:</span> <span class="token number">1e-8</span> <span class="token operator">*</span> <span class="token number">10</span><span class="token operator">**</span><span class="token punctuation">(</span>epoch <span class="token operator">/</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>lr<span class="token operator">=</span><span class="token number">1e-8</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><br>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">"mse"</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">)</span><br>history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> callbacks<span class="token operator">=</span><span class="token punctuation">[</span>lr_schedule<span class="token punctuation">]</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></code></pre><div class=columns-2><pre class=language-python><code class=language-python>lrs <span class="token operator">=</span> <span class="token number">1e-8</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">10</span> <span class="token operator">**</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>plt<span class="token punctuation">.</span>semilogx<span class="token punctuation">(</span>lrs<span class="token punctuation">,</span> history<span class="token punctuation">.</span>history<span class="token punctuation">[</span><span class="token string">"loss"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1e-8</span><span class="token punctuation">,</span> <span class="token number">1e-3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><img alt="Loss w.r.t different learning rates." class="img-100 bg-white" src=/img/post/mooc/tf/c4_w2_lr.png><br><em>Loss w.r.t different learning rates. We choose the lowest one, around 8e-6.</em></div><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-2/notebook_4_DNN_synthetic_data.html>DNN with synthetic TS</a>.<h2 id=rnn-for-ts tabindex=-1>RNN for TS <a href=#rnn-for-ts class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><p class=noindent><ul><li>RRN is a NN containing Recurrent layer.<li>The different from DNN is the input shape is <strong>3 dimensional</strong> (<code>batch_size x #time_step x dims_input_at each_timestep</code>).<li>Re-use 1 cell multiple times in different layers (in this course).</ul><p><img alt="Idea of how RNN works with TS data." class=img-100 src=/img/post/mooc/tf/rnn_ts_idea.png><br><em>Idea of how RNN works with TS data. The current location can be impacted more by the nearby locations.</em><h3 id=shape-of-input-to-rnn tabindex=-1>Shape of input to RNN <a href=#shape-of-input-to-rnn class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>ðŸ‘‰ <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/fP3ND/shape-of-the-inputs-to-the-rnn>Video explains the dimensional and sequence-to-vector RNN</a>.<p class=noindent><ul><li>Suppose: <em>window size</em> of 30 time steps, <em>batch size</em> of 4: Shape will be 4x30x1 and the <em>memory cell</em> input will be 4x1 matrix.<li>If the memory cell comprises 3 neurons then the <em>output matrix</em> will be 4x3. Therefore, the full output of the layer will be 4x30x3.<li><span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>H</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>H_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.08125em;>H</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em;><span style=top:-2.55em;margin-left:-0.0813em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>â€‹</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span></span></span></span></span> is just a copy of <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>Y_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.22222em;>Y</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3117em;><span style=top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s>â€‹</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span></span></span></span></span>.<li>Below figure: input and also output a sequence.</ul><p><img alt="Dimension of input to RNN." class=img-100 src=/img/post/mooc/tf/rnn_ts_dim.png><br><em>Dimension of input to RNN.</em><h3 id=sequence-to-vector-rnn tabindex=-1>Sequence to vector RNN <a href=#sequence-to-vector-rnn class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p class=noindent><ul><li>Sometimes, we want only input a sequence but not output. This called <strong>sequence-to-vector RNN</strong>. I.E., <mark>ignore all of the outputs except the last one!</mark>. In <code>tf.keras</code>, it's default setting!</ul><p><img alt="Sequence to vector RNN." class=img-100 src=/img/post/mooc/tf/rnn_ts_sequence_to_vector.png><br><em>Sequence to vector RNN.</em><pre class=language-python><code class=language-python><span class="token comment"># Check the figure below as an illustration</span><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SimpleRNN<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment"># input_shape:</span><br>    <span class="token comment">#   TF assumes that 1st dim is batch size -> any size at all -> no need to define</span><br>    <span class="token comment">#   None -> number of time steps, None means RNN can handle sequence of any length</span><br>    <span class="token comment">#   1 -> univariate TS</span><br>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SimpleRNN<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment"># if there is `return_sequences=True` -> sequence-to-sequence RNN</span><br>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><img alt="Illustration with keras." class=img-80 src=/img/post/mooc/tf/rnn_ts_illustraction_with_keras.png><br><em>Illustration with keras.</em><h3 id=lambda-layer tabindex=-1>Lambda layer <a href=#lambda-layer class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>ðŸ‘‰ <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/I0K6b/lambda-layers>Video explains the use of lambda layer in RNN.</a>.<pre class=language-python><code class=language-python>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> tf<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># expand to 1 dim (from 2) so that we have 3 dims: batch size x #timesteps x series dim</span><br>                        input_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># can use any size of sequences</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SimpleRNN<span class="token punctuation">(</span><span class="token number">40</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SimpleRNN<span class="token punctuation">(</span><span class="token number">40</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">*</span> <span class="token number">100.0</span><span class="token punctuation">)</span><br>        <span class="token comment"># default activation in RNN is tanh -> (-1, 1) -> scale to -100, 100</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id=simple-rnn tabindex=-1>Simple RNN <a href=#simple-rnn class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p class=noindent><ul><li>Loss function <strong>Huber</strong> (<a href=https://en.wikipedia.org/wiki/Huber_loss>wiki</a>): less sensitive to outliers. => we use this because our data in this case get a little bit noisy!</ul><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-3/notebook_1_simple_RNN_with_TS.html>Simple RNN with a TS data</a> + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/5W1Rw/rnn>videos explains it</a>.<h3 id=lstm tabindex=-1>LSTM <a href=#lstm class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-3/notebook_2_LSTM_with_TS.html>LSTM with a TS data</a> + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/IqcpX/more-on-lstm>videos explains it</a>.<pre class=language-python><code class=language-python><span class="token comment"># clear internal variables</span><br>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span><br>dataset <span class="token operator">=</span> windowed_dataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle_buffer_size<span class="token punctuation">)</span><br><br>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> tf<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>                        input_shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment"># LSTM here</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    <span class="token comment">#</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">*</span> <span class="token number">100.0</span><span class="token punctuation">)</span><br><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-3/notebook_3_LSTM_synthetic_data.html>LSTM with synthetic TS</a>.<h2 id=real-world-time-series-data tabindex=-1>Real-world time series data <a href=#real-world-time-series-data class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><p class=noindent><ul><li>We are going to predict the <strong>sunspot actitivity cycles</strong> (<a href=https://www.kaggle.com/robervalt/sunspots>download dataset</a>).<li>Combine CNN + LSTM.</ul><p>ðŸ‘‰ Andrew's <a href="https://www.youtube.com/watch?v=4qJaSmvhxi8">video on Optimization Algo: Mini-batch gradient descent</a>.<br>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-4/notebook_1_sunspot_cnn_lstm.html>Sunspot dataset with CNN+LSTM</a>. + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/zAaeD/combining-our-tools-for-analysis>video explains it</a>.<br>ðŸ“™ Notebook: <a href=https://dinhanhthi.github.io/tools/github-html?https://github.com/dinhanhthi/deeplearning.ai-courses/blob/master/TensorFlow%20in%20Practice/course-4/week-4/notebook_2_sunspot_DNN_only.html>Sunspot dataset with DNN only</a> + <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/EPaeW/sunspots>explaining video</a>.<br>ðŸ‘‰ <a href=https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/lecture/LYbcx/train-and-tune-the-model>Video explains train & tune the model</a> (how to choose suitable values for sizes)</div><script type=application/ld+json>{
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": "TF 4 - Sequences, TS and Prediction",
        "image": [],
        "author": "Anh-Thi DINH",
        "url": "https://dinhanhthi.com/deeplearning-ai-tensorflow-course-4/",
        "mainEntityOfPage": "https://dinhanhthi.com/deeplearning-ai-tensorflow-course-4/",
        "datePublished": "14-10-2020",
        "dateModified": "2020-10-14",
        "description": "This is my note for the 4th course of TensorFlow in Practice Specialization given by deeplearning.ai and taught by Laurence Moroney on..."
      }</script></article></main><footer><a href=/ target=_blank>Thi Â Â©Â  2022 </a>Â â€¢Â  <a href=/about-the-notes/ >About this site </a>Â â€¢Â  <a href=https://photos.app.goo.gl/9OVEkdTjmtRPg7vC3 target=_blank>My sketches </a>Â â€¢Â  <a href=https://goo.gl/photos/yQXdQws1LLS16x5v5 target=_blank>I cook </a>Â â€¢Â  <a href=/support-thi/ ><img alt="Support Thi" class=keep-original src=/img_src/icons/coffee.svg height=16 width=auto> Support Thi</a></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/elasticlunr/0.9.6/elasticlunr.min.js></script><script src=/src/js/components/clipboard.min.js></script><script src="/src/js/main.min.js?hash=d3b51a19b7" defer async></script><a href=/support-thi/ class="floating-button tooltip" id=buy-me-a-coffee><img alt="Support Thi" class=keep-original src=/img_src/icons/coffee.svg> <span class=tooltiptext>Support Thi</span> </a><button class="floating-button tooltip" id=scroll-top-btn><img alt="Scroll to top" class=keep-original src=/img_src/to-top.webp> <span class=tooltiptext>Top</span></button>