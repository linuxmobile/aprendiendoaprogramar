<!doctype html><html domain=.com lang=en><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="" http-equiv=Content-Security-Policy><link href=/favicon.svg rel=icon type=image/svg+xml><meta content=#f9c412 name=theme-color><meta content="max-snippet:-1, max-image-preview: large, max-video-preview: -1" name=robots><title>Principal Component Analysis (PCA) | Note of Thi</title><meta content="Principal Component Analysis (PCA) | Note of Thi" prefix=og:http://ogp.me/ns# property=og:title><meta content="üëâ Note: UMAP &amp;amp; t-SNE. What? Sometimes we need to &amp;quot;compress&amp;quot; our data to speed up algorithms or to visualize data. One way is..." name=description><meta content="üëâ Note: UMAP &amp;amp; t-SNE. What? Sometimes we need to &amp;quot;compress&amp;quot; our data to speed up algorithms or to visualize data. One way is..." prefix=og:http://ogp.me/ns# property=og:description><meta content=summary_large_image name=twitter:card><meta content=@dinhanhthi name=twitter:site><meta content=@dinhanhthi name=twitter:creator><meta content=https://dinhanhthi.com/img_src/cover.png prefix=og:http://ogp.me/ns# property=og:image><meta content=article prefix=og:http://ogp.me/ns# property=og:type><link href=https://dinhanhthi.com/principal-component-analysis/ rel=canonical><meta content=https://dinhanhthi.com/principal-component-analysis/ prefix=og:http://ogp.me/ns# property=og:url><meta content=no-referrer-when-downgrade name=referrer><link href=/feed/feed.xml rel=alternate type=application/atom+xml title="üî• Anh-Thi DINH"><link href=/ rel=preconnect crossorigin><link href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css rel=stylesheet crossorigin=anonymous integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET><script src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js defer crossorigin=anonymous integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb></script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js defer crossorigin=anonymous integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl onload=renderMathInElement(document.body);></script><style>body {
      visibility: hidden;
      opacity: 0;
    }</style><noscript><style>body {
        visibility: visible;
        opacity: 1;
      }</style></noscript><script csp-hash>if (/Mac OS X/.test(navigator.userAgent))
      document
        .documentElement
        .classList
        .add('apple')</script><link href=/src/css/main.css rel=stylesheet type=text/css><style>@font-face{font-display:swap;font-family:'fontello';src:url(/fontello/font/fontello.eot?46432155);src:url(/fontello/font/fontello.eot?46432155#iefix) format('embedded-opentype'),url(/fontello/font/fontello.woff2?46432155) format('woff2'),url(/fontello/font/fontello.woff?46432155) format('woff'),url(/fontello/font/fontello.ttf?46432155) format('truetype'),url(/fontello/font/fontello.svg?46432155#fontello) format('svg');font-weight:400;font-style:normal}[class*=" icon-"]:before,[class^=icon-]:before{font-family:"fontello";font-style:normal;font-weight:400;speak:never;display:inline-block;text-decoration:inherit;width:1em;margin-right:.2em;text-align:center;font-variant:normal;text-transform:none;line-height:1em;margin-left:.2em;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.icon-doc:before{content:'\e800'}.icon-doc-add:before{content:'\e801'}.icon-stats:before{content:'\e802'}.icon-tags:before{content:'\e803'}.icon-like:before{content:'\e804'}.icon-cog-alt:before{content:'\e805'}.icon-project:before{content:'\e806'}.icon-mooc:before{content:'\e807'}.icon-dl:before{content:'\e808'}.icon-ml:before{content:'\e809'}.icon-python:before{content:'\e80a'}.icon-nlp:before{content:'\e80b'}.icon-r-lang:before{content:'\e80c'}.icon-skill:before{content:'\e80d'}.icon-js-solid:before{content:'\e80e'}.icon-game:before{content:'\e80f'}.icon-web:before{content:'\e810'}.icon-algo:before{content:'\e811'}.icon-mooc-solid:before{content:'\e812'}.icon-chatbot:before{content:'\e813'}.icon-web-dev:before{content:'\e814'}.icon-data:before{content:'\e815'}.icon-skill-solid:before{content:'\e816'}.icon-python-solid:before{content:'\e817'}.icon-project-solid:before{content:'\e818'}.icon-js:before{content:'\e819'}.icon-rocket:before{content:'\e81a'}.icon-ml-solid:before{content:'\e81b'}.icon-game-solid:before{content:'\e81c'}.icon-dl-solid:before{content:'\e81d'}.icon-nlp-solid:before{content:'\e81e'}.icon-web-solid:before{content:'\e81f'}.icon-algo-solid:before{content:'\e820'}.icon-puzzle-outline:before{content:'\e821'}.icon-ng:before{content:'\e822'}.icon-ok:before{content:'\e823'}.icon-link:before{content:'\e824'}.icon-down-circle:before{content:'\e826'}.icon-api:before{content:'\e828'}.icon-right-circle:before{content:'\e829'}.icon-down-open:before{content:'\e82a'}.icon-right-open:before{content:'\e82b'}.icon-copy:before{content:'\f0c5'}.icon-gamepad:before{content:'\f11b'}.icon-fork:before{content:'\f126'}.icon-mlops-solid:before{content:'\f135'}.icon-edu-solid:before{content:'\f19d'}.icon-others-solid:before{content:'\f1b3'}.icon-ds-solid:before{content:'\f1c0'}.icon-chart-area:before{content:'\f1fe'}.icon-chart-pie:before{content:'\f200'}.icon-ts:before{content:'\f201'}.icon-clone:before{content:'\f24d'}</style><body><script>function showTheme() {
        const btn = document.getElementById("toggle-dark-light");
        let toggleIcon = btn.firstElementChild;
        const currentTheme = localStorage.getItem("theme");
        if (currentTheme === "dark") {
          document
            .body
            .classList
            .toggle("dark-theme");
          toggleIcon.src = "/img_src/nav/sun.svg";
        } else if (currentTheme === "light") {
          document
            .body
            .classList
            .toggle("light-theme");
          toggleIcon.src = "/img_src/nav/moon.svg";
        }
      }
      function showContent() {
        document.body.style.visibility = 'visible';
        document.body.style.opacity = 1;
      }
      window.addEventListener('DOMContentLoaded', (event) => {
        showTheme();
        showContent();
      });</script><header class="wave-border wave-border-post"><nav><div id=nav><a href=/ class="nav-item no-effect"><img alt=home class=keep-original src=/img_src/nav/home.svg height=18 width=18> <span>Thi</span> </a><a href=/about/ class="nav-item no-effect"><img alt=about class=keep-original src=/img_src/nav/about.svg height=15 width=15> <span>About</span></a><div class=nav-search id=nav-search><form><input aria-label='search notes (press "/" to focus & "ESC" to lose)' autocomplete=off class=nav-search__input id=nav-search__input onfocusin=inFocus(this) placeholder='search notes (press "/" to focus & "ESC" to lose)' type=search></form><div id=nav-search__result-container style="display: none;"><ul id=nav-search__ul></ul><div id=nav-search__no-result style="display: none;"><p>No results found.</div></div></div><span class="nav-item no-effect nav-dark-light" href="" id=toggle-dark-light><img alt=light-mode class=keep-original src=/img_src/nav/moon.svg height=20 width=20> </span><a href=https://github.com/dinhanhthi class="nav-item no-effect nav-github" target=_blank><img alt=github class=keep-original src=/img_src/nav/github.svg height=20 width=20></a></div><div class=reading-progress-container><div id=reading-progress aria-hidden=true></div></div></nav><script>var divNavSearch=document.getElementById("nav-search"),divRes=document.getElementById("nav-search__result-container"),ulRes=document.getElementById("nav-search__ul");function inFocus(e){""!=e.value&&(divRes.style.display="block")}var isOnDiv=!1;divRes.addEventListener("mouseover",(function(){isOnDiv=!0})),divRes.addEventListener("mouseout",(function(){isOnDiv=!1}));var inputSearch=document.getElementById("nav-search__input");window.addEventListener("click",(function(){ulRes.getElementsByTagName("li").length>=1&&isOnDiv||(divRes.style.display="none")})),inputSearch.addEventListener("click",(e=>{e.stopPropagation()}));const addSelected=e=>{ulRes.querySelectorAll("li").forEach((e=>{e.classList.remove("selected")})),e.classList.add("selected")};var isInView=(e,t)=>{var n=e.offsetTop+e.offsetHeight-t.scrollTop>t.offsetHeight;return e.offsetTop<t.scrollTop?"above":n?"below":"in"};const updateScroll=(e,t)=>{e.offsetTop+e.offsetHeight-t.scrollTop>t.offsetHeight&&(t.scrollTop=e.offsetTop+e.offsetHeight-t.offsetHeight),e.offsetTop<t.scrollTop&&(t.scrollTop=e.offsetTop)};document.onkeydown=e=>{checkInInput=document.activeElement==inputSearch,"/"!==e.key||checkInInput||(e.stopPropagation(),e.preventDefault(),inputSearch.focus())},document.addEventListener("focusin",(e=>{divNavSearch.contains(e.target)||(divRes.style.display="none")})),inputSearch.onkeydown=e=>{if("Enter"===e.key){e.stopPropagation(),e.preventDefault();var t=ulRes.querySelector('li[class*="selected"]');window.location.href=t.getElementsByClassName("item__content")[0].firstChild.firstChild.href}"Escape"===e.key&&(divRes.style.display="none",inputSearch.blur())},divNavSearch.onkeydown=e=>{if(hasResult=ulRes.getElementsByTagName("li").length>=1,hasResult){["ArrowUp","ArrowDown"].indexOf(e.key)>-1&&e.preventDefault();var t=ulRes.firstChild,n=ulRes.lastChild,s=ulRes.querySelector('li[class*="selected"]');switch(e.key){case"ArrowUp":nextLi=s&&s!=t?s.previousSibling:n,addSelected(nextLi),s=nextLi,updateScroll(s,divRes);break;case"ArrowDown":nextLi=s&&s!=n?s.nextSibling:t,addSelected(nextLi),s=nextLi,updateScroll(s,divRes)}}};</script><div class=header-container><div class="header-logo post-layout"><img alt="Principal Component Analysis (PCA)" class=keep-original src=/img/cats/ml.svg height=55 width=55></div><h1>Principal Component Analysis (PCA)</h1><div id=more-info><div id=note-tag><a href=/tags/machine-learning/ id=category }>Machine Learning</a> <a href=/tags/dimensionality-reduction/ id=category }>Dimensionality Reduction</a></div><div id=last-modified>Last modified 3 years ago / <a href=https://github.com/dinhanhthi/notes/edit/master/./posts/ml/2019-10-14-principal-component-analysis.md>Edit on Github</a></div></div></div></header><main class="" id=main-wrapper><article class=post-container><div class="container mt-2 normal page-note"><div class="danger not-full-warning"><div class=warning-icon><img alt=Warning class=keep-original src=/img_src/icons/time.svg></div><div>This post was <strong>updated more than 1 year ago</strong>, some information may be outdated!</div></div><div class="toc toc-common toc-js"><div class=ol-container><div class=toc-heading>In this note</div><ol><li><a href=#what%3F>What?</a><ol><li><a href=#algorithm>Algorithm</a></ol><li><a href=#code>Code</a><ol><li><a href=#whitening>Whitening</a></ol><li><a href=#pca-in-action>PCA in action</a><li><a href=#references>References</a></ol></div></div><p>üëâ Note: <a href=/umap-t-SNE/ >UMAP & t-SNE</a>.<h2 id=what%3F tabindex=-1>What? <a href=#what%3F class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><p>Sometimes we need to "compress" our data to speed up algorithms or to visualize data. One way is to use <a href=/tags/dimensionality-reduction/ ><strong>dimensionality reduction</strong></a> which is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. We can think of 2 approaches:<ul><li><strong>Feature selection</strong>: find a subset of the input variables.<li><strong>Feature projection</strong> (also <em>Feature extraction</em>): transforms the data in the high-dimensional space to a space of fewer dimensions. <strong>PCA</strong> is one of the methods following this approach.</ul><p><img alt="An idea of using PCA from 2D to 1D." class="pop img-full-90" src=/img/post/ML/dim_redu/pca-1.jpg><br><em><strong>Figure 1.</strong> An idea of using PCA from 2D to 1D.</em><p><img alt="An idea of using PCA from 5D to 2D." class="pop img-full-75" src=/img/post/ML/dim_redu/pca-2.jpg><br><em><strong>Figure 2.</strong> An idea of using PCA from 5D to 2D.</em><div class=warning><p>‚ùì <strong class=tbrown>Questions</strong>: How can we choose the <strong class=tgreen>green arrows</strong> like in Figure 1 and 2 (their <strong>directions</strong> and their <strong>magnitudes</strong>)?</div><p>From a data points, there are many ways of projections, for examples,<p><img alt="An example of different projections." class="pop img-full-75" src=/img/post/ML/dim_redu/pca-4.jpg><br><em><strong>Figure 3.</strong> We will project the points to the green line or the violet line? Which one is the best choice?</em><p>Intuitively, the green line is <mark>better with more separated points</mark>. But how can we choose it "mathematically" (precisely)? We need to know about:<ul><li><strong><a href=/mean-median-mode>Mean</a></strong>: finds the most balanced point in the data.<li><strong><a href=/variance-covariance-correlation>Variance</a></strong>: measures the spread of data from the mean. However, variance is not enough. There are many different ways in that we get the same variance.<li><strong><a href=/variance-covariance-correlation>Covariance</a></strong>: indicates the direction in that data are spreading.</ul><div class=hsbox><div class=hs__title>An example of the same mean and variance but different covariance.</div><div class=hs__content><p><img alt="Different data but the same mean and variance." class="pop img-full-100" src=/img/post/ML/dim_redu/pca-5.jpg><br><em><strong>Figure 4.</strong> Different data but the same mean and variance. That's why we need covariance!</em></div></div><h3 id=algorithm tabindex=-1>Algorithm <a href=#algorithm class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><ol><li><p>Subtract the mean to move to the original axes.<li><p>From the original data (a lot of features <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator=true>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=true>,</mo><mo>‚Ä¶</mo><mo separator=true>,</mo><msub><mi>x</mi><mi>N</mi></msub></mrow><annotation encoding=application/x-tex>x_1, x_2, \ldots, x_N</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.625em;vertical-align:-0.1944em;></span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em;><span style=top:-2.55em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>‚Äã</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em;><span style=top:-2.55em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>‚Äã</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=minner>‚Ä¶</span><span class=mspace style=margin-right:0.1667em;></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=mord><span class="mord mathnormal">x</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em;><span style=top:-2.55em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mathnormal mtight" style=margin-right:0.10903em;>N</span></span></span></span><span class=vlist-s>‚Äã</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span></span></span></span></span>), we construct a <strong>covariance matrix <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>U</mi></mrow><annotation encoding=application/x-tex>U</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.10903em;>U</span></span></span></span></span></strong>.<li><p>Find the <strong class=tbrown>eigenvalues</strong> <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>Œª</mi><mn>1</mn></msub><mo separator=true>,</mo><msub><mi>Œª</mi><mn>2</mn></msub><mo separator=true>,</mo><mo>‚Ä¶</mo></mrow><annotation encoding=application/x-tex>\lambda_1, \lambda_2,\ldots</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8889em;vertical-align:-0.1944em;></span><span class=mord><span class="mord mathnormal">Œª</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em;><span style=top:-2.55em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>‚Äã</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=mord><span class="mord mathnormal">Œª</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em;><span style=top:-2.55em;margin-left:0em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>‚Äã</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=minner>‚Ä¶</span></span></span></span></span> and correspondent <strong>eigenvectors</strong> <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>v</mi><mn>1</mn></msub><mo separator=true>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo separator=true>,</mo><mo>‚Ä¶</mo></mrow><annotation encoding=application/x-tex>v_1, v_2, \ldots</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.625em;vertical-align:-0.1944em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em;>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em;><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mtight">1</span></span></span></span><span class=vlist-s>‚Äã</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.03588em;>v</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3011em;><span style=top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mtight">2</span></span></span></span><span class=vlist-s>‚Äã</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span><span class=mpunct>,</span><span class=mspace style=margin-right:0.1667em;></span><span class=minner>‚Ä¶</span></span></span></span></span> of that matrix (we call them <strong>eigenstuffs</strong>). Choose <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>K</mi><mo>&lt;</mo><mi>N</mi></mrow><annotation encoding=application/x-tex>K &lt; N</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7224em;vertical-align:-0.0391em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>K</span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>&lt;</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.10903em;>N</span></span></span></span></span> couples <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>Œª</mi></mrow><annotation encoding=application/x-tex>\lambda</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6944em;></span><span class="mord mathnormal">Œª</span></span></span></span></span> and <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>v</mi></mrow><annotation encoding=application/x-tex>v</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.4306em;></span><span class="mord mathnormal" style=margin-right:0.03588em;>v</span></span></span></span></span> (the highest eigenvalues) and we get a reduced matrix <em><span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><msub><mi>U</mi><mi>K</mi></msub></mrow><annotation encoding=application/x-tex>U_K</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8333em;vertical-align:-0.15em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em;>U</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.3283em;><span style=top:-2.55em;margin-left:-0.109em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mathnormal mtight" style=margin-right:0.07153em;>K</span></span></span></span><span class=vlist-s>‚Äã</span></span><span class=vlist-r><span class=vlist style=height:0.15em;><span></span></span></span></span></span></span></span></span></span></span></em>.<li><p>Projection original data points to the <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>K</mi></mrow><annotation encoding=application/x-tex>K</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>K</span></span></span></span></span>-dimensional plane created based on these new <em>eigenstuffs</em>. This step creates new data points on a new dimensional space (<span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>K</mi></mrow><annotation encoding=application/x-tex>K</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>K</span></span></span></span></span>).<section class=eqn><eqn><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML display=block><semantics><mrow><mi>Z</mi><mo>=</mo><msubsup><mi>U</mi><mi>K</mi><mi>T</mi></msubsup><mi>X</mi></mrow><annotation encoding=application/x-tex>Z = U_K^TX</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>Z</span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:1.1383em;vertical-align:-0.247em;></span><span class=mord><span class="mord mathnormal" style=margin-right:0.10903em;>U</span><span class=msupsub><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:0.8913em;><span style=top:-2.453em;margin-left:-0.109em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mathnormal mtight" style=margin-right:0.07153em;>K</span></span></span><span style=top:-3.113em;margin-right:0.05em;><span class=pstrut style=height:2.7em;></span><span class="mtight reset-size6 size3 sizing"><span class="mord mathnormal mtight" style=margin-right:0.13889em;>T</span></span></span></span><span class=vlist-s>‚Äã</span></span><span class=vlist-r><span class=vlist style=height:0.247em;><span></span></span></span></span></span></span><span class="mord mathnormal" style=margin-right:0.07847em;>X</span></span></span></span></span></eqn></section><li><p>Now, instead of solving the original problem (<span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>N</mi></mrow><annotation encoding=application/x-tex>N</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.10903em;>N</span></span></span></span></span> features), we only need to solve a new problem with <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>K</mi></mrow><annotation encoding=application/x-tex>K</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>K</span></span></span></span></span> features (<span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>K</mi><mo>&lt;</mo><mi>N</mi></mrow><annotation encoding=application/x-tex>K&lt;N</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.7224em;vertical-align:-0.0391em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>K</span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>&lt;</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.10903em;>N</span></span></span></span></span>).</ol><p><img alt="A big picture of the idea of PCA algorithm." class="pop img-full-100" src=/img/post/ML/dim_redu/pca-3.jpg><br><em><strong>Figure 5.</strong> A big picture of the idea of PCA algorithm. "Eigenstuffs" are eigenvalues and eigenvectors. <a href="https://www.youtube.com/watch?v=g-Hb26agBFg">Source</a>.</em><h2 id=code tabindex=-1>Code <a href=#code class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><pre class=language-python><code class=language-python><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> PCA<br><br>s <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><br>pca <span class="token operator">=</span> PCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">150</span><span class="token punctuation">,</span> whiten<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span><br><span class="token comment"># pca.fit(s)</span><br>s1 <span class="token operator">=</span> pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>s<span class="token punctuation">)</span><br><br><span class="token keyword">print</span> <span class="token punctuation">(</span>pca<span class="token punctuation">.</span>components_<span class="token punctuation">)</span> <span class="token comment"># eigenvectors</span><br><span class="token keyword">print</span> <span class="token punctuation">(</span>pca<span class="token punctuation">.</span>explained_variance_<span class="token punctuation">)</span> <span class="token comment"># eigenvalues</span></code></pre><p>Some notable components (see <a href=https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html>full</a>):<ul><li><code>pca.fit(X)</code>: only fit <code>X</code> (and then we can use <code>pca</code> for other operations).<li><code>pca.fit_transform(X)</code>: Fit the model with <code>X</code> and apply the dimensionality reduction on <code>X</code> (from <code>(n_samples, n_features)</code> to <code>(n_samples, n_components)</code>).<li><code>pca.inverse_transform(s1)</code>: transform <code>s1</code> back to original data space (2D) - not back to <code>s</code>!!!<li><code>pca1.mean_</code>: mean point of the data.<li><code>pca.components_</code>: eigenvectors (<code>n_components</code> vectors).<li><code>pca.explained_variance_</code>: eigenvalues. It's also the amount of retained variance which is corresponding to <strong>each</strong> components.<li><code>pca.explained_variance_ratio_</code>: the <strong>percentage</strong> in that variance is retained if we consider on <strong>each</strong> component.</ul><p>Some notable parameters:<ul><li><code>n_components=0.80</code>: means it will return the Eigenvectors that have the 80% of the variation in the dataset.</ul><div class=warning><p>When choosing the number of principal components (<span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>K</mi></mrow><annotation encoding=application/x-tex>K</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>K</span></span></span></span></span>), we choose <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>K</mi></mrow><annotation encoding=application/x-tex>K</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>K</span></span></span></span></span> to be the smallest value so that for example, <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>99</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>99\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em;></span><span class=mord>99%</span></span></span></span></span> of variance, is retained.<sup><a href=https://stackoverflow.com/questions/32857029/python-scikit-learn-pca-explained-variance-ratio-cutoff target=_blank rel="noopener noreferrer">[ref]</a></sup><p>In <a href=https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html>Scikit-learn</a>, we can use <code>pca.explained_variance_ratio_.cumsum()</code>. For example, <code>n_components = 5</code> and we have,<pre class=language-python><code class=language-python><span class="token punctuation">[</span><span class="token number">0.32047581</span>  <span class="token number">0.59549787</span>  <span class="token number">0.80178824</span>  <span class="token number">0.932976</span>    <span class="token number">1.</span><span class="token punctuation">]</span></code></pre><p>then we know that with <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>K</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=application/x-tex>K=4</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>K</span><span class=mspace style=margin-right:0.2778em;></span><span class=mrel>=</span><span class=mspace style=margin-right:0.2778em;></span></span><span class=base><span class=strut style=height:0.6444em;></span><span class=mord>4</span></span></span></span></span>, we would retain <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mn>93.3</mn><mi mathvariant=normal>%</mi></mrow><annotation encoding=application/x-tex>93.3\%</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.8056em;vertical-align:-0.0556em;></span><span class=mord>93.3%</span></span></span></span></span> of the variance.</div><h3 id=whitening tabindex=-1>Whitening <a href=#whitening class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h3><p>Whitening makes the features:<ul><li>less correlated with each other,<li>all features have the same variance (or, unit component-wise variances).</ul><p><img alt="An illustration of whitening." class="pop img-full-100" src=/img/post/ML/dim_redu/pca-6.jpeg><br><em>PCA / Whitening. <strong>Left</strong>: Original toy, 2-dimensional input data. <strong>Middle</strong>: After performing PCA. The data is centered at zero and then rotated into the eigenbasis of the data covariance matrix. This decorrelates the data (the covariance matrix becomes diagonal). <strong>Right</strong>: Each dimension is additionally scaled by the eigenvalues, transforming the data covariance matrix into the identity matrix. Geometrically, this corresponds to stretching and squeezing the data into an isotropic gaussian blob.</em><p>If this section doesn't satisfy you, read <a href=http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/#whitening>this</a> and <a href=http://cs231n.github.io/neural-networks-2/ >this</a> (section <em>PCA and Whitening</em>).<h2 id=pca-in-action tabindex=-1>PCA in action <a href=#pca-in-action class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><ul><li><p><strong>Example to understand the idea of PCA</strong>: <a href=/files/ml/pca/PCA_understanding_example.html>html file</a> -- <a href=https://colab.research.google.com/drive/1F_A_fJOY-oiV7Ly4y_evF9sfwII-ljJK>open in colab</a>.<ul><li>Plot points with 2 lines which are corresponding to 2 eigenvectors.<li>Plot & choose Principal Components.<li>An example of choosing <code>n_components</code> <span class=eq><span class=katex><span class=katex-mathml><math xmlns=http://www.w3.org/1998/Math/MathML><semantics><mrow><mi>K</mi></mrow><annotation encoding=application/x-tex>K</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:0.6833em;></span><span class="mord mathnormal" style=margin-right:0.07153em;>K</span></span></span></span></span>.<li>Visualization hand-written digits (the case of all digits and the case of only 2 digits -- 1 & 8).<li>Using <a href=/support-vector-machine>SVM</a> to classifier data in the case of 1 & 8 and visualize the decision boundaries.</ul><li><p><strong>Image compression</strong>: <a href=/files/ml/pca/PCA-image-compression.html>html file</a> -- <a href=https://colab.research.google.com/drive/1G_WPZMmQ020kjSmqMI_k21_zLDrPlYtg>open in colab</a>.<ul><li>When input is an image, the values of adjacent pixels are <em>highly correlated</em>.<li>Import images from <code>scipy</code> and Google Drive or Github (with <code>git</code>).<li>Compress grayscale images and colored ones.<li>Plot a grayscale version of a colorful images.<li>Save output to file (Google Drive).<li>Fix warning <em>Lossy conversion from float64 to uint8. Range [...,...]. Convert image to uint8 prior to saving to suppress this warning.</em><li>Fix warning <em>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)</em>.<li>Calculate a size (in <code>KB</code>) of a image file.</ul><li><p><strong>PCA without scikit-learn</strong>: <a href=/files/ml/pca/PCA_without_scikit_learn.html>html file</a> -- <a href=https://colab.research.google.com/drive/1IWMuon3NSpGybmnBBWxlvbS9yUjxtf_8>open in colab</a>.</ul><h2 id=references tabindex=-1>References <a href=#references class=direct-link aria-hidden=true><i class="fontello-icon icon-link"></i></a></h2><ul><li><strong>Luis Serrano</strong> -- [Video] <a href="https://www.youtube.com/watch?v=g-Hb26agBFg">Principal Component Analysis (PCA)</a>. It's very intuitive!<li><strong>Stats.StackExchange</strong> -- <a href=https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues>Making sense of principal component analysis, eigenvectors & eigenvalues</a>.<li><strong>Scikit-learn</strong> -- <a href=https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html>PCA official doc</a>.<li><strong>Tiep Vu</strong> -- <em>Principal Component Analysis</em>: <a href=https://machinelearningcoban.com/2017/06/15/pca/ >B√†i 27</a> and <a href=https://machinelearningcoban.com/2017/06/21/pca2/ >B√†i 28</a>.<li><strong>Jake VanderPlas</strong> -- <a href=https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html>In Depth: Principal Component Analysis</a>.<li><strong>Tutorial 4 Yang</strong> -- <a href=/files/ml/pca/tutorial4-yang.pdf>Principal Components Analysis</a>.<li><strong>Andrew NG.</strong> -- <a href=https://rawnote.dinhanhthi.com/machine-learning-coursera-8#principal-component-analysis-pca>My raw note</a> of the course <a href=https://www.coursera.org/learn/machine-learning/ >"Machine Learning" on Coursera</a>.<li><strong>Shankar Muthuswamy</strong> -- <a href=https://shankarmsy.github.io/posts/pca-sklearn.html>Facial Image Compression and Reconstruction with PCA</a>.<li><strong>UFLDL - Stanford</strong> -- <a href=http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/ >PCA Whitening</a>.</ul></div><script type=application/ld+json>{
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": "Principal Component Analysis (PCA)",
        "image": [],
        "author": "Anh-Thi DINH",
        "url": "https://dinhanhthi.com/principal-component-analysis/",
        "mainEntityOfPage": "https://dinhanhthi.com/principal-component-analysis/",
        "datePublished": "14-10-2019",
        "dateModified": "2019-10-14",
        "description": "üëâ Note: UMAP &amp;amp; t-SNE. What? Sometimes we need to &amp;quot;compress&amp;quot; our data to speed up algorithms or to visualize data. One way is..."
      }</script></article></main><footer><a href=/ target=_blank>Thi ¬†¬©¬† 2022 </a>¬†‚Ä¢¬† <a href=/about-the-notes/ >About this site </a>¬†‚Ä¢¬† <a href=https://photos.app.goo.gl/9OVEkdTjmtRPg7vC3 target=_blank>My sketches </a>¬†‚Ä¢¬† <a href=https://goo.gl/photos/yQXdQws1LLS16x5v5 target=_blank>I cook </a>¬†‚Ä¢¬† <a href=/support-thi/ ><img alt="Support Thi" class=keep-original src=/img_src/icons/coffee.svg height=16 width=auto> Support Thi</a></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/elasticlunr/0.9.6/elasticlunr.min.js></script><script src=/src/js/components/clipboard.min.js></script><script src="/src/js/main.min.js?hash=d3b51a19b7" defer async></script><a href=/support-thi/ class="floating-button tooltip" id=buy-me-a-coffee><img alt="Support Thi" class=keep-original src=/img_src/icons/coffee.svg> <span class=tooltiptext>Support Thi</span> </a><button class="floating-button tooltip" id=scroll-top-btn><img alt="Scroll to top" class=keep-original src=/img_src/to-top.webp> <span class=tooltiptext>Top</span></button>